{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Requirements:\n",
    "!pip install rtdl\n",
    "!pip install libzero==0.0.4"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import rtdl\n",
    "import sklearn.datasets\n",
    "import sklearn.model_selection\n",
    "import sklearn.preprocessing\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import zero"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "device = torch.device('cpu')\n",
    "# Docs: https://yura52.github.io/zero/0.0.4/reference/api/zero.improve_reproducibility.html\n",
    "zero.improve_reproducibility(seed=123456)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# !!! NOTE !!! The dataset splits, preprocessing and other details are\n",
    "# significantly different from those used in the\n",
    "# paper \"Revisiting Deep Learning Models for Tabular Data\",\n",
    "# so the results will be different from the reported in the paper.\n",
    "dataset = sklearn.datasets.fetch_california_housing()\n",
    "X_all = dataset['data'].astype('float32')\n",
    "y_all = dataset['target'].astype('float32')\n",
    "X = {}\n",
    "y = {}\n",
    "X['train'], X['test'], y['train'], y['test'] = sklearn.model_selection.train_test_split(\n",
    "    X_all, y_all, train_size=0.8\n",
    ")\n",
    "X['train'], X['val'], y['train'], y['val'] = sklearn.model_selection.train_test_split(\n",
    "    X['train'], y['train'], train_size=0.8\n",
    ")\n",
    "\n",
    "# not the best way to preprocess features, but enough for the demonstration\n",
    "preprocess = sklearn.preprocessing.StandardScaler().fit(X['train'])\n",
    "X = {\n",
    "    k: torch.tensor(preprocess.fit_transform(v), device=device)\n",
    "    for k, v in X.items()\n",
    "}\n",
    "\n",
    "# !!! CRUCIAL for neural networks when solving regression problems !!!\n",
    "y_mean = float(y['train'].mean())\n",
    "y_std = float(y['train'].std())\n",
    "y = {\n",
    "    k: torch.tensor((v - y_mean) / y_std, device=device)\n",
    "    for k, v in y.items()\n",
    "}"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Model\n",
    "Carefully read the comments and uncomment the code for the model you want to test."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# model = rtdl.MLP.make_baseline(\n",
    "#     d_in=X_all.shape[1],\n",
    "#     d_layers=[128, 256, 128],\n",
    "#     dropout=0.1,\n",
    "#     d_out=1\n",
    "# )\n",
    "# lr = 0.001\n",
    "# weight_decay = 0.0\n",
    "\n",
    "# model = rtdl.ResNet.make_baseline(\n",
    "#     d_in=X_all.shape[1],\n",
    "#     d_main=128,\n",
    "#     d_intermidiate=256,\n",
    "#     dropout_first=0.2,\n",
    "#     dropout_second=0.0,\n",
    "#     n_blocks=2,\n",
    "#     d_out=1\n",
    "# )\n",
    "# lr = 0.001\n",
    "# weight_decay = 0.0\n",
    "\n",
    "model = rtdl.FTTransformer.make_default(\n",
    "    n_num_features=X_all.shape[1],\n",
    "    cat_cardinalities=None,\n",
    "    last_layer_query_idx=[-1],  # it makes the model faster and does NOT affect its output\n",
    "    d_out=1,\n",
    ")\n",
    "\n",
    "# === ABOUT CATEGORICAL FEATURES ===\n",
    "# IF you use MLP, ResNet or any other simple feed-forward model (NOT transformer-based model)\n",
    "# AND there are categorical features\n",
    "# AND you want to transform the categorical features to embeddings\n",
    "# THEN continue reading this comment.\n",
    "# ==================================\n",
    "# 1. When you have both numerical and categorical features, you should prepare you data like this:\n",
    "#    (X_num<float32>, X_cat<int64>) instead of X<float32>\n",
    "#    Each column in X_cat should contain values within the range from 0 to <(the number of unique values in column) - 1>;\n",
    "#    use sklean.preprocessing.OrdinalEncoder to achieve this;\n",
    "# 2. Prepare a list of so called \"cardinalities\":\n",
    "#    cardinalities[i] = <the number of unique values of the i-th categorical feature>\n",
    "# 3. Uncomment the following snippet and set `d_token` to any appropriate value\n",
    "#\n",
    "# class Model(nn.Module):\n",
    "#     def __init__(self, embedding: rtdl.FlatEmbedding, model: nn.Module):\n",
    "#         super().__init__()\n",
    "#         self.embedding = embedding\n",
    "#         self.model = model\n",
    "#\n",
    "#     def forward(self, x_num, x_cat):\n",
    "#         return self.model(self.embedding(x_num, x_cat))\n",
    "#\n",
    "# model = Model(\n",
    "#     # `None` means \"Do not transform numerical features\"\n",
    "#     # `d_token` is the size of embedding for ONE categorical feature\n",
    "#     rtdl.FlatEmbedding(None, rtdl.CategoricalFeatureTokenizer(cardinalities, d_token, True, 'uniform')),\n",
    "#     base_model  # a model such as MLP, ResNet, etc.\n",
    "# )\n",
    "# Then the model should be used as `model(x_num, x_cat)` instead of of `model(x)`.\n",
    "\n",
    "model.to(device)\n",
    "optimizer = (\n",
    "    model.make_default_optimizer()\n",
    "    if isinstance(model, rtdl.FTTransformer)\n",
    "    else torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    ")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Training"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def apply_model(x_num, x_cat=None):\n",
    "    # rtdl.FTTransformer expects two inputs: x_num and x_cat\n",
    "    return model(x_num, x_cat) if isinstance(model, rtdl.FTTransformer) else model(x_num)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(part):\n",
    "    model.eval()\n",
    "    # hopefully, the whole dataset fits in memory\n",
    "    mse = F.mse_loss(apply_model(X[part]).squeeze(1), y[part]).item()\n",
    "    rmse = mse ** 0.5 * y_std\n",
    "    return rmse\n",
    "\n",
    "\n",
    "# Create a dataloader for batches of indices\n",
    "# Docs: https://yura52.github.io/zero/reference/api/zero.data.IndexLoader.html\n",
    "batch_size = 256\n",
    "train_loader = zero.data.IndexLoader(len(X['train']), batch_size, device=device)\n",
    "\n",
    "# Create a progress tracker for early stopping\n",
    "# Docs: https://yura52.github.io/zero/reference/api/zero.ProgressTracker.html\n",
    "progress = zero.ProgressTracker(patience=100)\n",
    "\n",
    "print(f'Test RMSE before training: {evaluate(\"test\"):.4f}')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "n_epochs = 1000\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    for batch_idx in train_loader:\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        x_batch = X['train'][batch_idx]\n",
    "        y_batch = y['train'][batch_idx]\n",
    "        F.mse_loss(apply_model(x_batch).squeeze(1), y_batch).backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    val_rmse = evaluate('val')\n",
    "    test_rmse = evaluate('test')\n",
    "    print(f'Epoch {epoch:03d} | Validation RMSE: {val_rmse:.4f} | Test RMSE: {test_rmse:.4f}', end='')\n",
    "    progress.update(-val_rmse)\n",
    "    if progress.success:\n",
    "        print(' <<< BEST VALIDATION EPOCH', end='')\n",
    "    print()\n",
    "    if progress.fail:\n",
    "        break"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}