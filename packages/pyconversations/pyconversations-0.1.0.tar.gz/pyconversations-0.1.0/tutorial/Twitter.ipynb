{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "unavailable-blast",
   "metadata": {},
   "source": [
    "# PyConversations: A Twitter-based Example\n",
    "\n",
    "The following is a tutorial notebook that demonstrates how to use `pyconversations` with Twitter data.\n",
    "\n",
    "In order to directly follow and run this tutorial notebook, you will need a valid set of Twitter API credentials.\n",
    "\n",
    "First, we'll obtain some data and then we'll show how to load that directly into PyConversations!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fitting-taylor",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, time, json, sys, os\n",
    "\n",
    "from datetime import datetime as dt\n",
    "\n",
    "from pyconversations.convo import Conversation\n",
    "from pyconversations.message import Tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aggressive-clothing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Place your App's Bearer token here:\n",
    "token = ''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "first-transition",
   "metadata": {},
   "source": [
    "This first portion of downloading code with ping and author and attempt to identify their most recent tweet ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fifth-intelligence",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_recent(author, bearer_token, fields='&tweet.fields=id'):\n",
    "    \"\"\"\n",
    "    Given an author and a token, attempts to return the ID of the Tweet\n",
    "    that this user most recently posted\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    author\n",
    "        A Twitter username\n",
    "    bearer_token \n",
    "        The API credentials\n",
    "    fields\n",
    "        Other associated fields for the request\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    The most recent Tweet ID\n",
    "    \"\"\"\n",
    "    # set the authentication header\n",
    "    headers = {\"Authorization\": \"Bearer {}\".format(bearer_token)}\n",
    "    \n",
    "    # access the tweet's data\n",
    "    tweet_url = \"https://api.twitter.com/2/tweets/search/recent?query=from:\" + author + fields\n",
    "    tweet_response = requests.request(\"GET\", tweet_url, headers=headers)\n",
    "    \n",
    "    return tweet_response.json()['data'][0]['id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "quarterly-valley",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1427329311556648961'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_recent_tid = download_recent('YouTube', token)\n",
    "most_recent_tid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "outstanding-effort",
   "metadata": {},
   "source": [
    "Next, we'll snag a batch of Tweets associated with this conversation.\n",
    "We could batch-read the entire Conversation by iteratively querying more tweets from this Conversation, however, here, we'll just take the first batch of Tweets returned from the API associated with the Conversation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "interested-leeds",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all the fields we'll want to extract from the conversation\n",
    "all_fields = (\"&tweet.fields=author_id,conversation_id,created_at,in_reply_to_user_id,referenced_tweets\" +\n",
    "              \",geo,lang,source,reply_settings,id,public_metrics\" +\n",
    "              \"&expansions=author_id,in_reply_to_user_id\" +\n",
    "              \"&user.fields=name,username\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "foster-thomson",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_thread(tid, bearer_token, fields=all_fields, max_results = '50'):\n",
    "    \"\"\"\n",
    "    Given an arbitrary Tweet ID `tid` and valid API credential `bearer_token`,\n",
    "    this function downloads an arbitrary Twitter thread associated with \n",
    "    the source Tweet `tid`.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    tid\n",
    "        A Tweet ID\n",
    "    bearer_token \n",
    "        The API credentials\n",
    "    fields\n",
    "        Other associated fields for the request\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    The data associated with the Twitter thread\n",
    "    \"\"\"\n",
    "    # set the authentication header\n",
    "    headers = {\"Authorization\": \"Bearer {}\".format(bearer_token)}\n",
    "    \n",
    "    # access the tweet's data\n",
    "    tweet_url = \"https://api.twitter.com/2/tweets?ids=\" + tid + fields\n",
    "    tweet_response = requests.request(\"GET\", tweet_url, headers=headers)\n",
    "    rate_limit_headers = {ky: tweet_response.headers[ky]\n",
    "                          for ky in ['x-rate-limit-reset', 'x-rate-limit-limit','x-rate-limit-remaining']}\n",
    "    data = [tweet_response.json()]\n",
    "    \n",
    "    conversation_url = ('https://api.twitter.com/2/tweets/search/recent?query=' +\n",
    "                        'conversation_id:' + data[0]['data'][0]['conversation_id'] +\n",
    "                        '&since_id='+str(int(data[0]['data'][0]['conversation_id']) - 1) +\n",
    "                        '&max_results=' + max_results + fields)\n",
    "    conversation_response = requests.request(\"GET\", conversation_url, headers=headers)\n",
    "    rate_limit_headers = {ky: conversation_response.headers[ky]\n",
    "                          for ky in ['x-rate-limit-reset', 'x-rate-limit-limit','x-rate-limit-remaining']}\n",
    "    data.append(conversation_response.json())\n",
    "        \n",
    "    # make sure we have the materials to continue  \n",
    "    if 'meta' in data[-1] and 'data' in data[-1]:\n",
    "        batch_size = data[-1]['meta']['result_count']\n",
    "        if 'next_token' not in data[-1]['meta']:\n",
    "            return data\n",
    "    else:\n",
    "        return data\n",
    "        \n",
    "    # finish up if return wasn't otherwise triggered\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "knowing-intranet",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = download_thread(most_recent_tid, token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "italian-happiness",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "serial-survivor",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# total tweets:\n",
    "sum([len(x['data']) for x in tweets])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "undefined-wesley",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data': [{'conversation_id': '1427329311556648961',\n",
       "   'text': 'RT if you always leave a comment',\n",
       "   'id': '1427329311556648961',\n",
       "   'source': 'Sprinklr',\n",
       "   'author_id': '10228272',\n",
       "   'lang': 'en',\n",
       "   'reply_settings': 'everyone',\n",
       "   'public_metrics': {'retweet_count': 223,\n",
       "    'reply_count': 152,\n",
       "    'like_count': 874,\n",
       "    'quote_count': 17},\n",
       "   'created_at': '2021-08-16T18:00:01.000Z'}],\n",
       " 'includes': {'users': [{'id': '10228272',\n",
       "    'name': 'YouTube',\n",
       "    'username': 'YouTube'}]}}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "norwegian-truth",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'conversation_id': '1427329311556648961',\n",
       " 'text': '@YouTube Whatâ€™s the point if cowards like dc and Disney  turn comments off ðŸ¤£',\n",
       " 'referenced_tweets': [{'type': 'replied_to', 'id': '1427329311556648961'}],\n",
       " 'id': '1427334250924285954',\n",
       " 'source': 'Twitter for iPhone',\n",
       " 'author_id': '1404903027992051712',\n",
       " 'lang': 'en',\n",
       " 'reply_settings': 'everyone',\n",
       " 'public_metrics': {'retweet_count': 0,\n",
       "  'reply_count': 0,\n",
       "  'like_count': 0,\n",
       "  'quote_count': 0},\n",
       " 'in_reply_to_user_id': '10228272',\n",
       " 'created_at': '2021-08-16T18:19:38.000Z'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets[1]['data'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "constant-quarterly",
   "metadata": {},
   "source": [
    "## Integration with `pyconversations`\n",
    "\n",
    "All that's left to do is plug our data directly into `pyconversations`!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "allied-enhancement",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This handles v2 API... Not yet fully integrated in PyConversations\n",
    "def fromutcformat(utc_str, tz=None):\n",
    "    iso_str = utc_str.replace('Z', '+00:00')\n",
    "    return dt.fromisoformat(iso_str).astimezone(tz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "alternative-salem",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create conversation\n",
    "conv = Conversation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "expressed-overhead",
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in tweets:\n",
    "    for t in batch['data']:\n",
    "        # create data for the post constructor\n",
    "        cons = {\n",
    "            'uid':        t['id'],\n",
    "            'created_at': fromutcformat(t['created_at']),\n",
    "            'text':       t['text'].strip(),\n",
    "            'author':     [u['username'] for u in batch['includes']['users']if u['id'] == t['author_id']][0],\n",
    "            'reply_to':   [r['id'] for r in t.get('referenced_tweets', [])],\n",
    "            'lang':       t['lang'],\n",
    "        }\n",
    "        conv.add_post(Tweet(**cons))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "common-indianapolis",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print number of unique posts contained within the Conversation\n",
    "len(conv.posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "effective-furniture",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('1427329311556648961',\n",
       "  Tweet(Twitter::YouTube::2021-08-16 14:00:01-04:00::RT if you always leave a comment::tags=)),\n",
       " ('1427334250924285954',\n",
       "  Tweet(Twitter::wba434::2021-08-16 14:19:38-04:00::@YouTube Whatâ€™s the point if cowards like dc and D::tags=)),\n",
       " ('1427334238567747598',\n",
       "  Tweet(Twitter::43Huntley::2021-08-16 14:19:35-04:00::@YouTube @TechWizYT L A::tags=)),\n",
       " ('1427334129008402432',\n",
       "  Tweet(Twitter::steviexrocker::2021-08-16 14:19:09-04:00::@YouTube I donâ€™t reply if someone always say â€œnice::tags=)),\n",
       " ('1427334111815950342',\n",
       "  Tweet(Twitter::TempGamers::2021-08-16 14:19:05-04:00::@YouTube https://t.co/eOHIUXlI7F::tags=))]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(conv.posts.items())[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "green-license",
   "metadata": {},
   "source": [
    "### Sub-Conversation Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "subsequent-convenience",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# seperate disjoint conversations\n",
    "segs = conv.segment()\n",
    "\n",
    "len(segs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unauthorized-joint",
   "metadata": {},
   "source": [
    "### (Detected) Language Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "available-occasions",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'en': 37, 'und': 13, 'lv': 1})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "lang_dist = Counter([post.lang for post in conv.posts.values()])\n",
    "lang_dist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intense-mayor",
   "metadata": {},
   "source": [
    "### Conversation-Level Redaction\n",
    "\n",
    "Using `Conversation.redact()` produces a thread that is cleaned of user-specific information. \n",
    "This is conversationally-scoped, so all usernames are first enumerated (either from author names or from in-text reference for Reddit and Twitter) and then user mentions (and author names) are replaced by `USER{\\d}` where `{\\d}` is the integer assigned to that username during the enumeration stage.\n",
    "\n",
    "Here's a demonstration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "applied-alloy",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(49,\n",
       " {'43Huntley',\n",
       "  'AnitaPa00338598',\n",
       "  'AsamnewAsamnew',\n",
       "  'DymeADuzin',\n",
       "  'Gamer_1745',\n",
       "  'GenuineTech80',\n",
       "  'Hollywood_652',\n",
       "  'ItzDodgerz',\n",
       "  'J_ffS0n',\n",
       "  'JosephYZSL',\n",
       "  'Kelashmeghwar1',\n",
       "  'KennyHylleberg',\n",
       "  'KhaledXKetata',\n",
       "  'KyepoTW',\n",
       "  'Le09hY',\n",
       "  'Leyoch234',\n",
       "  'ManakMazhar',\n",
       "  'MrJayMick',\n",
       "  'MuhammadShiblu4',\n",
       "  'Princehooper_',\n",
       "  'RykersToybox',\n",
       "  'Sara_AM_23',\n",
       "  'Scifiz1',\n",
       "  'Senistro_Band',\n",
       "  'Spikiie1',\n",
       "  'StanReinhardt',\n",
       "  'TJeffy125',\n",
       "  'TempGamers',\n",
       "  'TheCrussVentsel',\n",
       "  'TheHoodedMan0',\n",
       "  'ThomasD25788825',\n",
       "  'UnqualifiedDude',\n",
       "  'VesuvianLevio',\n",
       "  'YouTube',\n",
       "  'cabrobst',\n",
       "  'finds_e',\n",
       "  'imbellete',\n",
       "  'kid_hummus',\n",
       "  'lexbts7',\n",
       "  'lucas80628444',\n",
       "  'nakedtruth_fact',\n",
       "  'sarikaa_sri',\n",
       "  'sliprings',\n",
       "  'sneha_chandra',\n",
       "  'steviexrocker',\n",
       "  'theeKenyan_Icon',\n",
       "  'unchangend',\n",
       "  'wba434',\n",
       "  'weeklychatter'})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pre-redaction \n",
    "names = {post.author for post in conv.posts.values()}\n",
    "\n",
    "len(names), names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "hazardous-stewart",
   "metadata": {},
   "outputs": [],
   "source": [
    "# redaction step\n",
    "conv.redact()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "indonesian-works",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(49,\n",
       " {'USER0',\n",
       "  'USER1',\n",
       "  'USER10',\n",
       "  'USER11',\n",
       "  'USER12',\n",
       "  'USER13',\n",
       "  'USER14',\n",
       "  'USER15',\n",
       "  'USER17',\n",
       "  'USER18',\n",
       "  'USER19',\n",
       "  'USER2',\n",
       "  'USER20',\n",
       "  'USER21',\n",
       "  'USER22',\n",
       "  'USER23',\n",
       "  'USER24',\n",
       "  'USER25',\n",
       "  'USER26',\n",
       "  'USER27',\n",
       "  'USER28',\n",
       "  'USER29',\n",
       "  'USER30',\n",
       "  'USER36',\n",
       "  'USER38',\n",
       "  'USER4',\n",
       "  'USER40',\n",
       "  'USER41',\n",
       "  'USER43',\n",
       "  'USER44',\n",
       "  'USER45',\n",
       "  'USER46',\n",
       "  'USER47',\n",
       "  'USER48',\n",
       "  'USER49',\n",
       "  'USER5',\n",
       "  'USER50',\n",
       "  'USER51',\n",
       "  'USER52',\n",
       "  'USER53',\n",
       "  'USER55',\n",
       "  'USER56',\n",
       "  'USER57',\n",
       "  'USER58',\n",
       "  'USER59',\n",
       "  'USER6',\n",
       "  'USER7',\n",
       "  'USER8',\n",
       "  'USER9'})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# post-redaction\n",
    "names = {post.author for post in conv.posts.values()}\n",
    "len(names), names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "informed-student",
   "metadata": {},
   "source": [
    "### Saving and Loading from the universal format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "animal-honduras",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "lesser-treasurer",
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving a conversation to disk\n",
    "# alternatively: save as a JSONLine file, where each line is a conversation!\n",
    "json.dump(conv.to_json(), open('twitter_conv.json', 'w+'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "focused-subdivision",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reloading directly from the JSON\n",
    "conv_reloaded = Conversation.from_json(json.load(open('twitter_conv.json')))\n",
    "len(conv_reloaded.posts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "descending-worcester",
   "metadata": {},
   "source": [
    "### Feature Extraction\n",
    "\n",
    "The remainder of this notebook exhibits some basic vectorization of features from conversations, posts, and users within this conversation. \n",
    "For more information, see the documentation for PyConversations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "false-harrison",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyconversations.feature_extraction import ConversationVectorizer\n",
    "from pyconversations.feature_extraction import PostVectorizer\n",
    "from pyconversations.feature_extraction import UserVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "useful-traffic",
   "metadata": {},
   "outputs": [],
   "source": [
    "convs = True\n",
    "# convs = False\n",
    "\n",
    "users = True\n",
    "# users = False\n",
    "\n",
    "posts = True\n",
    "# posts = False\n",
    "\n",
    "# normalization = None\n",
    "# normalization = 'minmax'\n",
    "# normalization = 'mean'\n",
    "normalization = 'standard'\n",
    "\n",
    "# cv = ConversationVectorizer(normalization=normalization, agg_user_fts=users, agg_post_fts=posts, include_source_user=True)\n",
    "pv = PostVectorizer(normalization=normalization, include_conversation=convs, include_user=users)\n",
    "# uv = UserVectorizer(normalization=normalization, agg_post_fts=posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "extreme-reasoning",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyconversations.feature_extraction.extractors.PostVectorizer at 0x117d5cf90>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cv.fit(conv=conv_reloaded)\n",
    "pv.fit(conv=conv_reloaded)\n",
    "# uv.fit(conv=conv_reloaded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "behavioral-ivory",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(51, 3317)\n"
     ]
    }
   ],
   "source": [
    "# cxs = cv.transform(conv=conv_reloaded)\n",
    "pxs = pv.transform(conv=conv_reloaded)\n",
    "# uxs = uv.transform(conv=conv_reloaded)\n",
    "\n",
    "# print(cxs.shape)\n",
    "print(pxs.shape)\n",
    "# print(uxs.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
