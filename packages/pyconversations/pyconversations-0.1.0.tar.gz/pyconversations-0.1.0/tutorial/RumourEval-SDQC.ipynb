{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "arranged-newton",
   "metadata": {},
   "source": [
    "# PyConversations in Application -- SemEval 2019 Task 7a\n",
    "## Classifying the Support Labels of Rumours\n",
    "\n",
    "This notebook gives a simple tutorial of using features extracted from PyConversations in a machine learning pipeline. \n",
    "Here, we apply PyConversations to [SemEval 2019 Task 7 - RumourEval](https://aclanthology.org/S19-2147.pdf) on sub-task A.\n",
    "The goal of the task is to classifying whether comments are (S)upporting, (D)enying, (Q)uerying, or (C)ommenting (thus, SDQC is a short name for this task type).\n",
    "This notebook takes a simplistic stab at this task using only descriptive features from PyCovnersations (e.g., no sematic vectors to augment the data in PyConversations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cubic-jamaica",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from pyconversations.convo import Conversation\n",
    "from pyconversations.feature_extraction import ConversationVectorizer\n",
    "from pyconversations.feature_extraction import PostVectorizer\n",
    "from pyconversations.feature_extraction import UserVectorizer\n",
    "\n",
    "from RumourEval import load_rumoureval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "electrical-satisfaction",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 3, 1, 2])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit a label encoder to the label types\n",
    "classes = ['comment', 'support', 'deny', 'query']\n",
    "\n",
    "le = LabelEncoder()\n",
    "le.fit(classes)\n",
    "\n",
    "le.transform(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "tutorial-electronics",
   "metadata": {},
   "outputs": [],
   "source": [
    "# augment with where this data lives locally on your machine\n",
    "DATA_PATH = '/Users/hsh28/data/rumoureval2019/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fleet-webmaster",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "reading train+dev: 100%|██████████| 5568/5568 [00:13<00:00, 420.29it/s]\n",
      "reading test: 100%|██████████| 1066/1066 [00:02<00:00, 477.60it/s]\n",
      "reading train: 100%|██████████| 728/728 [00:05<00:00, 136.65it/s]\n",
      "reading dev: 100%|██████████| 446/446 [00:03<00:00, 117.57it/s]\n",
      "reading test: 100%|██████████| 761/761 [00:04<00:00, 170.10it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "8534"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# see previous tutorial if you have questions about where this code came from!\n",
    "dataset = load_rumoureval(DATA_PATH)\n",
    "len(dataset.posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "married-serve",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_split(dataset, split):\n",
    "    \"\"\"\n",
    "    Simple function for splitting out the different cuts of the data\n",
    "    \"\"\"\n",
    "    return Conversation(posts={pid: dataset.posts[pid] for pid in dataset.filter(by_tags={f'split={split}'})})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "olympic-strike",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': 5217, 'dev': 1485, 'test': 1827}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# split the posts\n",
    "keys = ['train', 'dev', 'test']\n",
    "splits = {k: get_split(dataset, k.upper()) for k in keys}\n",
    "\n",
    "{s: len(splits[s].posts) for s in splits}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "spiritual-alignment",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build different vectorizers\n",
    "\n",
    "# norm = None  # NOT RECOMMENDED\n",
    "# norm = 'standard'\n",
    "norm = 'minmax'\n",
    "# norm = 'mean'\n",
    "\n",
    "cvec = ConversationVectorizer(normalization=norm)\n",
    "pvec = PostVectorizer(normalization=norm)\n",
    "uvec = UserVectorizer(normalization=norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "mighty-contract",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': 327, 'dev': 40, 'test': 86}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# split out data at the conversational level for more feature availability\n",
    "convos = {s: splits[s].segment() for s in splits}\n",
    "{s: len(convos[s]) for s in convos}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "greater-assault",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ConvVec: Fitting by conversations: 100%|██████████| 327/327 [05:25<00:00,  1.01it/s]  \n",
      "PostVec: Fitting by conversations: 100%|██████████| 327/327 [03:32<00:00,  1.54it/s]  \n",
      "UserVec: Fitting by user: 100%|██████████| 3427/3427 [04:58<00:00, 11.48it/s] \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pyconversations.feature_extraction.extractors.UserVectorizer at 0x1235dffd0>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit the vectorizers to the training split for normalization\n",
    "k = 'train'\n",
    "cvec.fit(convos[k])\n",
    "pvec.fit(convos[k])\n",
    "uvec.fit(convos[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "three-questionnaire",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ConvVec: Transforming by conversations: 100%|██████████| 327/327 [05:22<00:00,  1.01it/s]  \n",
      "PostVec: Transforming by conversations: 100%|██████████| 327/327 [03:32<00:00,  1.54it/s]  \n",
      "UserVec: Transforming by users: 100%|██████████| 3427/3427 [05:08<00:00, 11.11it/s] \n",
      "ConvVec: Transforming by conversations: 100%|██████████| 40/40 [04:53<00:00,  7.35s/it]\n",
      "PostVec: Transforming by conversations: 100%|██████████| 40/40 [04:08<00:00,  6.21s/it]\n",
      "UserVec: Transforming by users: 100%|██████████| 1022/1022 [04:48<00:00,  3.54it/s]\n",
      "ConvVec: Transforming by conversations: 100%|██████████| 86/86 [08:56<00:00,  6.24s/it]  \n",
      "PostVec: Transforming by conversations: 100%|██████████| 86/86 [06:12<00:00,  4.33s/it]  \n",
      "UserVec: Transforming by users: 100%|██████████| 1277/1277 [07:46<00:00,  2.74it/s] \n"
     ]
    }
   ],
   "source": [
    "# transform all splits of the dataset into the vector and ID mappings \n",
    "\n",
    "cvs = {}\n",
    "pvs = {}\n",
    "uvs = {}\n",
    "cids = {}\n",
    "pids = {}\n",
    "uids = {}\n",
    "\n",
    "for k, cxs in convos.items():\n",
    "    v, i = cvec.transform(cxs, include_ids=True)\n",
    "    cvs[k] = v\n",
    "    cids[k] = i\n",
    "    \n",
    "    v, i = pvec.transform(cxs, include_ids=True)\n",
    "    pvs[k] = v\n",
    "    pids[k] = i\n",
    "    \n",
    "    v, i = uvec.transform(cxs, include_ids=True)\n",
    "    uvs[k] = v\n",
    "    uids[k] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "peripheral-helmet",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_input(cxs, pvs, cvs, uvs, pids, cids, uids):\n",
    "    \"\"\"\n",
    "    This function builds vectors for each post for SDQC classification.\n",
    "    Here, we naively produce vectors that contain:\n",
    "    * all features for the post in question\n",
    "    * all features for the conversation the post is in\n",
    "    * all features for the user who wrote the post\n",
    "    \n",
    "    This may not be the best feature set but is here as a demonstration \n",
    "    of how one might construct more complex feature vectors beyond \n",
    "    simple vectorization using built-in vectorizers.\n",
    "\n",
    "    For example, it might be advantageous to use information about the parent post\n",
    "    or source post and their users as well.\n",
    "    \"\"\"\n",
    "    pdim = pvs.shape[1]\n",
    "    cdim = cvs.shape[1]\n",
    "    udim = uvs.shape[1]\n",
    "    \n",
    "    # @ post-level\n",
    "    xs = np.zeros((len(pids), pdim + cdim + udim)) # (post, post_vec + convo_vec + author_of_post_vec)\n",
    "    ys = np.zeros(len(pids))  # (post,)\n",
    "    \n",
    "    for cx in tqdm(cxs, desc='Building XY-pairs'):\n",
    "        cid = cx.convo_id\n",
    "        for pid in cx.posts:\n",
    "            ix = pids[(cid, pid)]\n",
    "            px = cx.posts[pid]\n",
    "            user = px.author\n",
    "            \n",
    "            # place post vector\n",
    "            xs[ix, :pdim] = pvs[ix, :]\n",
    "            off = pdim\n",
    "            \n",
    "            # place conversation vector\n",
    "            xs[ix, off:cdim + off] = cvs[cids[cid], :]\n",
    "            off += cdim\n",
    "            \n",
    "            # place author's user vector\n",
    "            xs[ix, off:] = uvs[uids[user], :]\n",
    "            \n",
    "            for t in px.tags:\n",
    "                if 'taskA' in t:\n",
    "                    lx = t.split('taskA=')[-1]\n",
    "                    ys[ix] = le.transform([lx])[0]\n",
    "    return xs, ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "functional-forestry",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building XY-pairs: 100%|██████████| 327/327 [00:00<00:00, 1534.97it/s]\n",
      "Building XY-pairs: 100%|██████████| 40/40 [00:00<00:00, 768.27it/s]\n",
      "Building XY-pairs: 100%|██████████| 86/86 [00:00<00:00, 1330.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train (5217, 2308) (5217,)\n",
      "dev (1485, 2308) (1485,)\n",
      "test (1827, 2308) (1827,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Construct XY pairs for each data split from vectorized data and maps\n",
    "xs = {}\n",
    "ys = {}\n",
    "for k in convos:\n",
    "    x, y = build_input(convos[k], pvs[k], cvs[k], uvs[k], pids[k], cids[k], uids[k])\n",
    "    xs[k] = x\n",
    "    ys[k] = y\n",
    "    \n",
    "    print(k, xs[k].shape, ys[k].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "banned-hometown",
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature selection on train, a simple model for dropping un-helpful features\n",
    "k = 'train'\n",
    "model = SGDClassifier(loss='log', eta0=1e-4, learning_rate='adaptive', n_jobs=-1, random_state=0, max_iter=10_000)\n",
    "selector = SelectFromModel(estimator=model).fit(xs[k], ys[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "lucky-intent",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.02911557573016249, 774)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print the selected threshold and count the retained features\n",
    "selector.threshold_, selector.get_support().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "italic-hayes",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': (5217, 774), 'dev': (1485, 774), 'test': (1827, 774)}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# trim x-data to recommended feature set\n",
    "xs_trimmed = {\n",
    "    k: selector.transform(xs[k])\n",
    "    for k in xs\n",
    "}\n",
    "\n",
    "{k: xs_trimmed[k].shape for k in xs_trimmed}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "foreign-radius",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:02<00:00,  1.50it/s]\n",
      "100%|██████████| 4/4 [00:04<00:00,  1.03s/it]\n",
      "100%|██████████| 4/4 [00:04<00:00,  1.13s/it]\n",
      " 75%|███████▌  | 3/4 [00:12<00:05,  5.29s/it]/Users/hsh28/PycharmProjects/PyConversations/venv/lib/python3.7/site-packages/sklearn/linear_model/_stochastic_gradient.py:577: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n",
      "100%|██████████| 4/4 [01:16<00:00, 19.10s/it]\n",
      "100%|██████████| 4/4 [00:03<00:00,  1.25it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.4024571153448402, 0.0001, 'perceptron')"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# a simple dev-based hyperparameter selection approach\n",
    "best_s = 0\n",
    "best_eta = None\n",
    "best_loss = None\n",
    "\n",
    "k = 'train'\n",
    "\n",
    "for loss in ['hinge', 'log', 'modified_huber', 'squared_hinge', 'perceptron']:\n",
    "    for eta in tqdm([1e-4, 1e-3, 1e-2, 1e-1]):\n",
    "        model = SGDClassifier(loss=loss, eta0=eta, learning_rate='adaptive', n_jobs=-1, random_state=0, max_iter=10_000)\n",
    "        model.fit(xs_trimmed[k], ys[k])\n",
    "        dev_preds = model.predict(xs_trimmed['dev'])\n",
    "        s = f1_score(ys['dev'], dev_preds, average='macro')\n",
    "    \n",
    "        if s > best_s:\n",
    "            best_s = s\n",
    "            best_eta = eta\n",
    "            best_loss = loss\n",
    "        \n",
    "best_s, best_eta, best_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "global-camping",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "train\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.81      0.82      0.82      3519\n",
      "         1.0       0.27      0.17      0.21       378\n",
      "         2.0       0.48      0.52      0.50       395\n",
      "         3.0       0.55      0.60      0.57       925\n",
      "\n",
      "    accuracy                           0.71      5217\n",
      "   macro avg       0.53      0.53      0.52      5217\n",
      "weighted avg       0.70      0.71      0.71      5217\n",
      "\n",
      "==================================================\n",
      "dev\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.85      0.87      0.86      1181\n",
      "         1.0       0.00      0.00      0.00        82\n",
      "         2.0       0.55      0.47      0.51       120\n",
      "         3.0       0.19      0.30      0.24       102\n",
      "\n",
      "    accuracy                           0.75      1485\n",
      "   macro avg       0.40      0.41      0.40      1485\n",
      "weighted avg       0.74      0.75      0.74      1485\n",
      "\n",
      "==================================================\n",
      "test\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.85      0.79      0.82      1476\n",
      "         1.0       0.01      0.01      0.01       101\n",
      "         2.0       0.48      0.46      0.47        93\n",
      "         3.0       0.25      0.45      0.32       157\n",
      "\n",
      "    accuracy                           0.70      1827\n",
      "   macro avg       0.40      0.43      0.41      1827\n",
      "weighted avg       0.73      0.70      0.71      1827\n",
      "\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# re-fit the data and observe results\n",
    "model = SGDClassifier(loss=best_loss, eta0=best_eta, learning_rate='adaptive', n_jobs=-1, random_state=0, max_iter=10_000)\n",
    "model.fit(xs_trimmed[k], ys[k])\n",
    "\n",
    "train_preds = model.predict(xs_trimmed['train'])\n",
    "dev_preds = model.predict(xs_trimmed['dev'])\n",
    "test_preds = model.predict(xs_trimmed['test'])\n",
    "\n",
    "print('='*50)\n",
    "print('train')\n",
    "print(classification_report(ys['train'], train_preds))\n",
    "print('='*50)\n",
    "print('dev')\n",
    "print(classification_report(ys['dev'], dev_preds))\n",
    "print('='*50)\n",
    "print('test')\n",
    "print(classification_report(ys['test'], test_preds))\n",
    "print('='*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "thick-guidance",
   "metadata": {},
   "source": [
    "Though the scores are low, they are better than half of the submissions to the original SemEval competition! Also, recall that there is no sematic information (e.g., word vectors) used in this very, very simple pipeline, so it's not too shabby to get these results!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prescription-polls",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
