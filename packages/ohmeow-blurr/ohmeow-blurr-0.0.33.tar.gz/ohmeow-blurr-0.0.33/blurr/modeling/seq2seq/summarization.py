# AUTOGENERATED! DO NOT EDIT! File to edit: nbs/11_modeling-seq2seq-summarization.ipynb (unless otherwise specified).

__all__ = ['BlearnerForSummarization']

# Cell
import torch
from transformers import *
from fastai.text.all import *

from ...utils import *
from ...data.core import get_blurr_tfm
from ...data.seq2seq.core import *
from ...data.seq2seq.summarization import *
from ..core import *
from .core import *

logging.set_verbosity_error()

# Cell
@delegates(Blearner.__init__)
class BlearnerForSummarization(Blearner):

    def __init__(self, dls, hf_model, **kwargs):
        super().__init__(dls, hf_model, **kwargs)

    @classmethod
    def get_model_cls(cls):
        return AutoModelForSeq2SeqLM

    @classmethod
    def _add_t5_prefix(cls, inp):
        return f'summarize: {inp}'

    @classmethod
    def get_metrics_cb(self):
        seq2seq_metrics = {
            'rouge': {
                'compute_kwargs': { 'rouge_types': ["rouge1", "rouge2", "rougeL"], 'use_stemmer': True },
                'returns': ["rouge1", "rouge2", "rougeL"]
            },
            'bertscore': {
                'compute_kwargs': { 'lang': 'en' },
                'returns': ["precision", "recall", "f1"]
            }
        }

        return HF_Seq2SeqMetricsCallback(custom_metrics=seq2seq_metrics)

    @classmethod
    def _create_learner(cls, data,
                        pretrained_model_name_or_path,
                        preprocess_func,
                        text_attr,
                        summary_attr,
                        max_length,
                        max_target_length,
                        dblock_splitter,
                        hf_tok_kwargs, text_gen_kwargs, dl_kwargs, learner_kwargs):

        # we need to find the architecture to ensure "mbart" specific tokenizer kwargs are included
        model_cls = cls.get_model_cls()
        model = model_cls.from_pretrained(pretrained_model_name_or_path)
        hf_arch = BLURR.get_model_architecture(type(model).__name__)

        if (hf_arch == 'mbart'):
            hf_tok_kwargs = { **{'src_lang': 'en_XX', 'tgt_lang': 'en_XX'}, **hf_tok_kwargs }

        # get our hf objects
        hf_arch, hf_config, hf_tokenizer, hf_model = BLURR.get_hf_objects(pretrained_model_name_or_path,
                                                                          model_cls=model_cls,
                                                                          tokenizer_kwargs=hf_tok_kwargs)

        # if we need to preprocess the raw data before creating our DataLoaders
        if (preprocess_func):
            data = preprocess_func(data, hf_arch, hf_config, hf_tokenizer, hf_model, text_attr, summary_attr)

        # update text generation kwargs
        if (text_gen_kwargs is None and hf_arch in ['bart', 't5']):
            text_gen_kwargs = hf_config.task_specific_params['summarization']

        # not all "summarization" parameters are for the model.generate method ... remove them here
        generate_func_args = list(inspect.signature(hf_model.generate).parameters.keys())
        for k in text_gen_kwargs.copy():
            if k not in generate_func_args: del text_gen_kwargs[k]

        # update our text generation kwargs for mbart
        if (hf_arch == 'mbart'):
            text_gen_kwargs = { **{'decoder_start_token_id': 'en_XX'}, **text_gen_kwargs }

        # define getters
        if (isinstance(data, pd.DataFrame)):
            get_x = Pipeline(funcs=[ColReader(text_attr)])
            get_y = ColReader(summary_attr)
        else:
            get_x = Pipeline(funcs=[ItemGetter(text_attr)])
            get_y = ItemGetter(summary_attr)

        if (hf_arch == 't5'):
            get_x.add(cls._add_t5_prefix)

        # define our DataBlock and DataLoaders
        before_batch_tfm = HF_Seq2SeqBeforeBatchTransform(hf_arch, hf_config, hf_tokenizer, hf_model,
                                                          max_length=max_length,
                                                          max_target_length=max_target_length,
                                                          text_gen_kwargs=text_gen_kwargs)

        blocks = (HF_Seq2SeqBlock(before_batch_tfm=before_batch_tfm), noop)
        dblock = DataBlock(blocks=blocks,
                           get_x=get_x,
                           get_y=get_y,
                           splitter=dblock_splitter)

        dls = dblock.dataloaders(data, **dl_kwargs.copy())

        # return BLearner instance
        learner_kwargs['splitter'] = learner_kwargs.pop('splitter', partial(seq2seq_splitter, arch=hf_arch))
        learner_kwargs['loss_func'] = learner_kwargs.pop('loss_func', CrossEntropyLossFlat())

        return cls(dls, hf_model, **learner_kwargs.copy())

    @classmethod
    def from_dataframe(cls, df, pretrained_model_name_or_path, preprocess_func=None,
                       text_attr='text',
                       summary_attr='summary',
                       max_length=None,
                       max_target_length=None,
                       dblock_splitter=ColSplitter(),
                       hf_tok_kwargs={}, text_gen_kwargs={}, dl_kwargs={}, learner_kwargs={}):

        return cls._create_learner(df,
                                   pretrained_model_name_or_path=pretrained_model_name_or_path,
                                   preprocess_func=preprocess_func,
                                   text_attr=text_attr,
                                   summary_attr=summary_attr,
                                   max_length=max_length,
                                   max_target_length=max_target_length,
                                   dblock_splitter=dblock_splitter,
                                   hf_tok_kwargs=hf_tok_kwargs, text_gen_kwargs=text_gen_kwargs,
                                   dl_kwargs=dl_kwargs, learner_kwargs=learner_kwargs)


    @classmethod
    def from_csv(cls, csv_file, pretrained_model_name_or_path, preprocess_func=None,
                 text_attr='text',
                 summary_attr='summary',
                 max_length=None,
                 max_target_length=None,
                 dblock_splitter=ColSplitter(),
                 hf_tok_kwargs={}, text_gen_kwargs={}, dl_kwargs={}, learner_kwargs={}):

        df = pd.read_csv(csv_file)

        return cls.from_dataframe(df,
                                  pretrained_model_name_or_path=pretrained_model_name_or_path,
                                  preprocess_func=preprocess_func,
                                  text_attr=text_attr,
                                  summary_attr=summary_attr,
                                  max_length=max_length,
                                  max_target_length=max_target_length,
                                  dblock_splitter=dblock_splitter,
                                  hf_tok_kwargs=hf_tok_kwargs, text_gen_kwargs=text_gen_kwargs,
                                  dl_kwargs=dl_kwargs, learner_kwargs=learner_kwargs)

    @classmethod
    def from_dictionaries(cls, ds, pretrained_model_name_or_path, preprocess_func=None,
                          text_attr='text',
                          summary_attr='summary',
                          max_length=None,
                          max_target_length=None,
                          dblock_splitter=RandomSplitter(),
                          hf_tok_kwargs={}, text_gen_kwargs={}, dl_kwargs={}, learner_kwargs={}):

        return cls._create_learner(ds,
                                   pretrained_model_name_or_path=pretrained_model_name_or_path,
                                   preprocess_func=preprocess_func,
                                   text_attr=text_attr,
                                   summary_attr=summary_attr,
                                   max_length=max_length,
                                   max_target_length=max_target_length,
                                   dblock_splitter=dblock_splitter,
                                   hf_tok_kwargs=hf_tok_kwargs, text_gen_kwargs=text_gen_kwargs,
                                   dl_kwargs=dl_kwargs, learner_kwargs=learner_kwargs)
