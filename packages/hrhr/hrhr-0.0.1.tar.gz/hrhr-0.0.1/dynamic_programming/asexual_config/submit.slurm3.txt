#!/bin/bash
#! Make sure all the SBATCH directives go in this section and no other commands
#! Name of the job:
#SBATCH -J optimal_HRHR
#! Project name for Cunniffe group, use SL2 for paying queue:
#SBATCH -A CUNNIFFE-SL3-CPU
#! Output filename:
#! %A means slurm job ID and %a means array index
#SBATCH --output=%A_%a.out 
#! Errors filename:
#SBATCH --error=%A_%a.err 
#! How many whole nodes should be allocated? (for single core jobs always leave this at 1)
#SBATCH --nodes=1
#! How many tasks will there be in total? (for single core jobs always leave this at 1)
#SBATCH --ntasks=1
#! How many many cores will be allocated per task? (for single core jobs always leave this at 1)
#SBATCH --cpus-per-task=1 
#! Estimated runtime: hh:mm:ss (job is force-stopped after if exceeded):
#! less than 12 hours on SL3 (non-paid)
#SBATCH --time=11:59:00
#! Estimated maximum memory needed (job is force-stopped if exceeded):
#! Never request less than 5980mb of memory. 
#! RAM is allocated in ~5980mb blocks, you are charged per block used, 
#! and unused fractions of blocks will not be usable by others.
#SBATCH --mem=5980mb
#! Submit a job array with index values between 0 and 31
#! NOTE: This must be a range, not a single number (i.e. specifying '32' here would only run one job, with index 32). Here top number is x=n_p-1 (0-x is then n_p iterations)
#SBATCH --array=0-31

#! This is the partition name. This will request for a node with 6GB RAM for each task
#SBATCH -p skylake

# mail alert at start, end and abortion of execution
#SBATCH --mail-type=ALL

#! Don't put any #SBATCH directives below this line, it is now safe to put normal commands below this line

#! Modify the environment seen by the application. For this example we need the default modules.
. /etc/profile.d/modules.sh                # This line enables the module command
module purge                               # Removes all modules still loaded
module load rhel7/default-peta4            # REQUIRED - loads the basic environment
module load use.own                        # This line loads the own module list
module load /rds/project/cag1/rds-cag1-general/epidem-modules/epidem.modules   # This line loads the Epidemiology group module list
module load python3

#! Command line that we want to run:
#! The variable $SLURM_ARRAY_TASK_ID contains the array index for each job.
#! In this example, each job will be passed its index, so each output file will contain a different value
python3.sh HR_HR/Scripts/Run2_optimal_asexual_cluster.py phi_con_$SLURM_ARRAY_TASK_ID 3