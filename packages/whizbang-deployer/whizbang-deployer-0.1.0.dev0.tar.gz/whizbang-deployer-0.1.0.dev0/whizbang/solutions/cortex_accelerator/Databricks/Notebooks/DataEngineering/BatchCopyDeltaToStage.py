{"version":"NotebookV1","origId":3273046978841371,"name":"BatchCopyDeltaToStage","language":"python","commands":[{"version":"CommandV1","origId":3273046978841372,"guid":"d08e3e7e-65f0-4d3a-a3b9-80858f03e6d0","subtype":"command","commandType":"auto","position":1.0,"command":"%md\n### Purpose\nThis notebook BATCH copies tables from DELTA to STAGE Zone for Polybase ingestion to DW.\n\n### Params\nAs a minimum, pass in the file reference that contains the tables to copy to Stage.","commandVersion":1,"state":"finished","results":{"type":"raw","data":"","arguments":{},"metadata":{}},"errorSummary":null,"errorTraceType":null,"error":null,"workflows":[],"startTime":0,"submitTime":0,"finishTime":0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":[],"yColumns":[],"pivotColumns":[],"pivotAggregation":null,"useConsistentColors":false,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","latestUserId":null,"commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"isLockedInExamMode":false,"iPythonMetadata":null,"streamStates":{},"datasetPreviewNameToCmdIdMap":{},"nuid":"3f984b68-2378-4776-b28e-7e9d63444398"},{"version":"CommandV1","origId":3273046978841373,"guid":"54832353-f9a6-4d80-ab93-e9a2fc5ff3d9","subtype":"command","commandType":"auto","position":2.0,"command":"%md\n####Libraries, Constants and Widgets (params)","commandVersion":1,"state":"finished","results":{"type":"raw","data":"","arguments":{},"metadata":{}},"errorSummary":null,"errorTraceType":null,"error":null,"workflows":[],"startTime":0,"submitTime":0,"finishTime":0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":[],"yColumns":[],"pivotColumns":[],"pivotAggregation":null,"useConsistentColors":false,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","latestUserId":null,"commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"isLockedInExamMode":false,"iPythonMetadata":null,"streamStates":{},"datasetPreviewNameToCmdIdMap":{},"nuid":"3ac2779e-066a-413c-8c6a-6f232d597c80"},{"version":"CommandV1","origId":3273046978841374,"guid":"7cbb8960-2258-42aa-88b6-6a8d0236eb0c","subtype":"command","commandType":"auto","position":3.0,"command":"from pyspark.sql.functions import max, length, upper, lower\nfrom pyspark.sql import functions as F\nfrom pyspark.sql.types import StringType\n\nfrom datetime import datetime\nimport json\n\nimport multiprocessing as mp #for parallelizing file ingestion\nimport os\n\nSTAGE_LOCATION = \"STAGE\"\n\n# Get current UTC time to be used for entire batch of files to be ingested\ndtUTC = datetime.utcnow()\nBATCH_TIME = dtUTC.isoformat() # UTC timestamp for type 2 column\nBATCH_TIME_STR = dtUTC.strftime('%Y-%m-%dT%H:%M:%S.%fZ') # UTC timestamp for MERGE commands\n\n# Get a count of cores (per worker) to use as a limit to parallel execution threads\n# (Can also use a multiplier to scale up, if desired)\nCORES = mp.cpu_count() \n\nCRITICAL = 'CRITICAL'\nERROR    = 'ERROR'\nWARNING  = 'WARNING'\nINFO     = 'INFO'\nDEBUG    = 'DEBUG'\n\n# Quoting constant for writing CSV with Pandas \nQUOTE_NONNUMERIC = 2\n\n# Initialize list that will contain critical errors for output\nbatch_errors = []\n\n# Get date parts for file path\nx = datetime.now()\nYYYY = (x.strftime(\"%Y\"))\nMM = (x.strftime(\"%m\"))\nDD = (x.strftime(\"%d\"))\nHHMMSS = (x.strftime(\"%H-%M-%Sz\")) #Matches format generated by ADF\n\n#dbutils.widgets.removeAll()\ndbutils.widgets.text(\"FileParams\",\"\")\ndbutils.widgets.dropdown(\"Execute Query\", \"TRUE\", [\"TRUE\", \"FALSE\"])\n\nfp = dbutils.widgets.get(\"FileParams\")\nEXECUTE_QUERY = eval(dbutils.widgets.get(\"Execute Query\").title())\n\nprint('Execute query:\\t', EXECUTE_QUERY)\nprint('Parameter file:\\t', fp)","commandVersion":1,"state":"finished","results":{"type":"listResults","data":[],"arguments":{},"addedWidgets":{},"removedWidgets":[],"datasetInfos":[],"metadata":{}},"errorSummary":null,"errorTraceType":null,"error":null,"workflows":[],"startTime":0,"submitTime":0,"finishTime":0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":[],"yColumns":[],"pivotColumns":[],"pivotAggregation":null,"useConsistentColors":false,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","latestUserId":null,"commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"isLockedInExamMode":false,"iPythonMetadata":null,"streamStates":{},"datasetPreviewNameToCmdIdMap":{},"nuid":"1ebb5dc6-1589-41dd-b4a8-a3a545312216"},{"version":"CommandV1","origId":3273046978841375,"guid":"b5c397bd-7500-447c-9107-60c3b670eb82","subtype":"command","commandType":"auto","position":4.0,"command":"%run /DataEngineering/SharedUtils","commandVersion":1,"state":"finished","results":{"type":"listResults","data":[],"arguments":{},"addedWidgets":{},"removedWidgets":[],"datasetInfos":[],"metadata":{}},"errorSummary":null,"errorTraceType":null,"error":null,"workflows":[],"startTime":0,"submitTime":0,"finishTime":0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":[],"yColumns":[],"pivotColumns":[],"pivotAggregation":null,"useConsistentColors":false,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","latestUserId":null,"commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"isLockedInExamMode":false,"iPythonMetadata":null,"streamStates":{},"datasetPreviewNameToCmdIdMap":{},"nuid":"108081f4-1b9f-478e-917b-0add26c590d2"},{"version":"CommandV1","origId":3273046978841376,"guid":"9d927ee2-c668-4f2a-8856-ad06db1ba1d0","subtype":"command","commandType":"auto","position":5.0,"command":"%md\n### Define Class and Methods for Copying Delta to Stage","commandVersion":1,"state":"finished","results":{"type":"raw","data":"","arguments":{},"metadata":{}},"errorSummary":null,"errorTraceType":null,"error":null,"workflows":[],"startTime":0,"submitTime":0,"finishTime":0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":[],"yColumns":[],"pivotColumns":[],"pivotAggregation":null,"useConsistentColors":false,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","latestUserId":null,"commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"isLockedInExamMode":false,"iPythonMetadata":null,"streamStates":{},"datasetPreviewNameToCmdIdMap":{},"nuid":"d7be890b-9084-4104-b71e-71070782bdb7"},{"version":"CommandV1","origId":3273046978841377,"guid":"7dcac767-2a88-437a-b57b-5e313f15eda4","subtype":"command","commandType":"auto","position":6.0,"command":"class DeltaToStage:\n    \"\"\"\n    Represents either a Delta table or Synapse table Polybase export\n    This is used as the source to copy to SnowFlake\n    \"\"\"\n    \n    def __init__(self, sourceSchema=None, sourceObject=None, filePath=None, executeQuery=True):\n        self.deltaSchemaName = sourceSchema.lower()\n        self.deltaObjectName = sourceObject.lower()\n        self.targetSchema = sourceSchema #same as source\n        self.targetObject = sourceObject #same as source\n        self.executeQuery = executeQuery\n        self.filePath = filePath\n        self.fullFilePath = self.get_full_file_path()\n        self.errorLog = []\n        self.startTimeUtc = datetime.utcnow()\n        self.endTimeUtc = None\n        self.success = True\n        self.df = None\n        self.schemaFile = \"\" #Set to empty string so it doesn't return \"None\" in output\n        \n    ###########################################################\n    # logger\n    ###########################################################\n    def logger(self, message, log_message=False, severity_level=INFO):\n        \"\"\"\n        A simple logger that adds timestamp and severity level to print message.\n        Optionally adds message to instance's \"log\".\n        \"\"\"\n        dtNow = datetime.utcnow()\n       \n        if len(message) > 0:\n            # Simple message to console\n            printMsg = f\"{dtNow:%Y-%m-%d %H:%M:%S}::{message}\"\n\n            # \"Thread-safe\" the print function, or some things print on the same line\n            print(f\"{printMsg}\\n\", end =\"\")\n\n        # Log entry for output\n        if log_message:\n            success = 1\n            if severity_level== CRITICAL:\n                success = 0\n            # Set end time\n            self.endTimeUtc = dtNow\n            # Build output message\n            logEntry = f\"{self.targetObject}|{self.startTimeUtc:%Y-%m-%d %H:%M:%S}|{self.endTimeUtc:%Y-%m-%d %H:%M:%S}|{self.schemaFile}|{success}|{message}\"\n            self.errorLog.append(logEntry)\n\n    ###########################################################\n    # get_full_file_path\n    ###########################################################\n    def get_full_file_path(self):\n        \"\"\"\n        Appends mount root (mnt) to filepath, ensuring slashes\n        are properly formatted.\n        \"\"\"\n        if self.filePath:\n            #Add Databricks/dbfs root mount point to file name\n            args = [\"mnt\", STAGE_LOCATION, self.filePath]\n            return slash_join(args, True)  # handles existing leading slash (or not)\n      \n    ###########################################################\n    # load_file\n    ###########################################################\n    def load_table(self):\n        \"\"\"\n        Creates a dataframe from the Delta base table\n        \"\"\"\n        try:\n            # Read in the table\n            self.df = spark.table(f\"{self.deltaSchemaName}.{self.deltaObjectName}\")\n\n            if self.df: \n                return True\n            # Shouldn't ever get here...\n            else:\n                return False\n        except:\n            out_msg = f\"ERROR: Unable to load source table: {self.deltaObjectName.upper()}\"\n            self.logger(out_msg, True, CRITICAL)\n            return False\n\n    ###########################################################\n    # write_table\n    ###########################################################\n    def write_table(self):\n        \"\"\"\n        Creates or overwrites the STAGE files\n        \"\"\"\n        self.logger(f\"Writing files for table {self.deltaObjectName} to: {self.filePath}.\")\n        try:\n            self.df.repartition(60).write.format('ORC').mode(\"overwrite\").option(\"compression\",\"snappy\").save(self.fullFilePath)\n\n            self.logger(f\"Files for table {self.deltaObjectName} have been written to: {self.filePath}.\")\n            return True\n        except:\n            out_msg = f\"ERROR: Unable to copy table to STAGE: {self.filePath}.\"\n            self.logger(out_msg, True, CRITICAL)\n            return False\n\n    ###########################################################\n    # get_schema_for_output\n    ###########################################################\n    def get_schema_for_output(self):\n        try:\n            import math\n            self.logger(f\"Generating schema from table {self.deltaObjectName} for output.\")\n\n            output_schema = []\n            str_schema = ''\n            varchar_len_list = [50, 100, 255, 500, 1000, 4000]\n\n            # Get the schema from source object\n            schema_list = self.df.dtypes\n  \n            # Iterate over columns, translating Spark types to SQL, and calculating string lengths\n            for i, c in enumerate(schema_list):\n                col_len = 0\n                col_name = c[0].strip()\n                sql_data_type = c[1]\n                spark_base_type = sql_data_type\n\n                # Find the position of \"(\", and store the data type and length to variables\n                pos = sql_data_type.find(\"(\")\n                col_len = \"\"\n                if pos > 0:\n                    spark_base_type = spark_base_type[:pos]\n                    col_len = sql_data_type[pos:]\n\n                # Map the Spark datatype to SQL\n                dtype = spark_to_sql_datatype(spark_base_type)\n\n                # If this is a string, we need to know the length, so get the max length from source data\n                # and then \"round up\" to nearest value in list defined above\n                if spark_base_type == 'string':\n                    col_spec = self.df.select(max(length(self.df[col_name])).alias(\"maxrowlen\")).rdd.flatMap(lambda x: x).collect()\n                    new_len = min(i for i in varchar_len_list if i >= (col_spec[0] or 0)) # handle null/None values\n                    output_schema.append(f\"[{col_name.strip()}] {dtype}({new_len})\")\n                else:\n                    output_schema.append(f\"[{col_name.strip()}] {dtype}{col_len}\")\n            \n            out_str = str(output_schema).strip('[]').replace(\"'\", \"\")\n            self.schemaFile = out_str\n            return True\n        except:\n            out_msg = f\"ERROR: Unable to generate output schema for table: {self.deltaObjectName}.\"\n            self.logger(out_msg, True, CRITICAL)\n            return False\n","commandVersion":1,"state":"finished","results":{"type":"listResults","data":[],"arguments":{},"addedWidgets":{},"removedWidgets":[],"datasetInfos":[],"metadata":{}},"errorSummary":null,"errorTraceType":null,"error":null,"workflows":[],"startTime":0,"submitTime":0,"finishTime":0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":[],"yColumns":[],"pivotColumns":[],"pivotAggregation":null,"useConsistentColors":false,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","latestUserId":null,"commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"isLockedInExamMode":false,"iPythonMetadata":null,"streamStates":{},"datasetPreviewNameToCmdIdMap":{},"nuid":"ae5e20e0-38e0-4acd-899d-009924f353ff"},{"version":"CommandV1","origId":3273046978841378,"guid":"67c8ce84-31f3-431d-8e6e-fc36c65c7d85","subtype":"command","commandType":"auto","position":7.0,"command":"%md\n### Notebook Functions (not Class methods)","commandVersion":1,"state":"finished","results":{"type":"raw","data":"","arguments":{},"metadata":{}},"errorSummary":null,"errorTraceType":null,"error":null,"workflows":[],"startTime":0,"submitTime":0,"finishTime":0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":[],"yColumns":[],"pivotColumns":[],"pivotAggregation":null,"useConsistentColors":false,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","latestUserId":null,"commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"isLockedInExamMode":false,"iPythonMetadata":null,"streamStates":{},"datasetPreviewNameToCmdIdMap":{},"nuid":"f4915d16-bd6c-4cef-9874-00b63c66b0d4"},{"version":"CommandV1","origId":3273046978841379,"guid":"39c5b75b-0608-405b-afe7-3ac3d0b8daf0","subtype":"command","commandType":"auto","position":8.0,"command":"###########################################################\n# load_batch_config\n# Parse the FileParams parameter to determine if it is JSON \n# or a reference to a CSV file, then load into dictionary\n###########################################################\ndef load_batch_config(config):\n    sdf = None\n\n    # Try to parse FileParams to see if it's JSON, if not, assume it's a file path\n    if config:\n        try:\n            file_params = json.loads(config)\n        except:\n            file_params = config\n            pass\n    else:\n        #raise ValueError(\"The fileParams parameter must be provided!\") \n        print(\"The fileParams parameter must be provided!\")\n        return None\n    \n    if file_params:\n        # First let's see if this parses as JSON\n        try:\n            # Use Pandas for easy conversion of JSON to dataframe\n            pdf = pd.DataFrame(file_params)\n            # Then convert to Spark Dataframe\n            sdf = spark.createDataFrame(pdf)\n\n        #Not JSON, so assuming a file reference\n        except:\n            #Add Databricks/dbfs root mount point to file name\n            args = [\"mnt\", file_params]\n            theFile = slash_join(args, True)  # handles existing leading slash (or not)\n            params_file = theFile\n\n           \n            if not file_exists(params_file):\n                return None\n\n            print(f\"{datetime.now():%Y-%m-%d %H:%M:%S}::Loading BATCH configuration file from {file_params}...\")\n\n            # Read the file\n            if params_file.endswith(\".csv\"):\n                sdf = (spark \n                            .read \n                            .format('csv') \n                            .option(\"header\", \"true\") \n                            .option(\"inferSchema\", \"true\") \n                            .load(params_file)\n                          )\n\n            if sdf.count() == 0:\n                return None\n\n        # Convert column names to lower case so that's not an issue\n        # Also convert data so folders/files will be lower case\n        sdf = sdf.select(*[lower(F.col(x)).alias(x.lower()) for x in sdf.columns])\n\n        if 'stagefolderpath' not in sdf.columns:\n            sdf = sdf.withColumn(\"stagefolderpath\", F.lit(None).cast(StringType()))\n\n        # Create a dictionary object from dataframe to iterate though\n        params_dict = dataframeToDict(sdf)\n\n    else:\n        params_dict = None\n\n    return params_dict\n  \n###########################################################\n# copy_table\n###########################################################\ndef copy_table(table):\n    \"\"\"\n    ***Used by parallel_copy function for parallelizing loads***\n    \"\"\"\n    table.logger(f\"Processing table {table.deltaSchemaName}.{table.deltaObjectName}...\")\n\n    # Load the file, then \n    # make sure the data actually loaded into the dataframe before proceeding.\n    if table.load_table():\n\n        if table.executeQuery:\n            # Write the table to Stage\n            if table.write_table():\n                if table.get_schema_for_output():\n                    errMsg = f\"{table.deltaObjectName} has been copied to STAGE zone: {table.filePath}\"\n                    table.logger(errMsg)\n                    # Log SUCCESS\n                    errMsg = \"\" #no message for success\n                    table.logger(errMsg, True, INFO) #True, so it will log it\n                    return \n        else:\n            errMsg = f\"TESTING:: Copy table: {table.deltaObjectName} to STAGE zone: {table.filePath}.\"\n            table.logger(errMsg)\n            return\n\n    else:\n        errMsg = f\"ERROR processing table {table.deltaSchemaName}.{table.deltaObjectName}\"\n        table.logger(errMsg, True, CRITICAL)\n        return\n\n###########################################################\n# write_output_file\n###########################################################\ndef write_output_file():\n    \"\"\"\n    Generate the output file with an entry for each file processed\n    \"\"\"\n    # Quoting constant for writing CSV with Pandas \n    QUOTE_NONNUMERIC = 2\n\n    col_list = ['DeltaTable','DeltaStartTimeUtc','DeltaEndTimeUtc','DeltaAdditionalOutput','DeltaSuccess','DeltaError']\n    # Return batch errors as JSON \n    if batch_errors:\n        pdf = pd.DataFrame(batch_errors, columns = col_list).reindex()\n\n        # Convert a couple columns to Ints\n        pdf['DeltaSuccess'] = pdf['DeltaSuccess'].astype(int)\n\n        # Generate error log file name, based on input file\n        outFile = fp.replace(\".csv\", \"_OUTPUTLOG.csv\")\n        # Add dbfs/mnt/ to file path\n        saveLocationParts = [\"dbfs\", \"mnt\", outFile]\n        saveLocation = slash_join(saveLocationParts, True) \n\n        # Write out file using Pandas for a single CSV file (without extra Spark metadata folder/files)\n        #try:\n        if not pdf.empty:\n            pdf[col_list].to_csv(saveLocation, sep=\"|\", index=False, quotechar='\"', quoting=QUOTE_NONNUMERIC)\n            outFile = outFile.replace(\"/dbfs/mnt/\", \"\")\n            print(f\"OUTPUT file written to: {outFile}\")\n    else:\n        outFile = \"\"\n    \n    return outFile\n###########################################################\n# parallel_copy\n###########################################################\ndef parallel_copy(tables, numInParallel=CORES):\n    \"\"\"\n    Main logic for parallel execution of table copy \n    Ref:  https://docs.python.org/3/library/concurrent.futures.html\n    \"\"\"\n    # This code limits the number of parallel executions.\n    # For each table (class) object, execute \"process_table\" function in parallel\n    with ThreadPoolExecutor(max_workers=numInParallel) as executor:\n        return [executor.submit(copy_table, table) for table in tables]\n\n###########################################################\n# process_tables\n###########################################################\ndef process_tables():\n  \n    print(f\"{datetime.now():%Y-%m-%d %H:%M:%S}::Batch Copy to STAGE zone starting...\")\n\n    # Create empty list that will contain class instances of each file to be ingested\n    tables = []\n\n    table_count = len(params_dict[\"sourceobject\"])\n    \n    # Create class instances for each file in the dictionary\n    for i in range(0, table_count):\n        source_schema = params_dict[\"sourceschema\"][i]\n        source_object = params_dict[\"sourceobject\"][i]\n        # Test to see if Stage folder path was passed in.  If so, use it.\n        if params_dict[\"stagefolderpath\"][i] != None:\n            file_path = params_dict[\"stagefolderpath\"][i]\n        else:\n            file_path = slash_join([source_schema, source_object, YYYY, MM, DD, HHMMSS], True)\n        print(\"file path:\", file_path)\n        \n        tables.append(\n                DeltaToStage(\n                  sourceSchema = source_schema,\n                  sourceObject = source_object,\n                  filePath = file_path,\n                  executeQuery = EXECUTE_QUERY\n                )\n        )\n    \n    # Parallel execution\n    res = parallel_copy(tables, CORES)\n    result = [f.result(timeout=3600) for f in res] # This is a blocking call.\n    \n    for table in tables:\n        if table.errorLog:\n            for err in table.errorLog:\n                batch_errors.append(err.split(\"|\"))\n\n    if fp:\n        # Generate output file\n        out = write_output_file()\n        print(f\"{datetime.now():%Y-%m-%d %H:%M:%S}::Batch copy to STAGE zone completed!\")\n        print(\"-----------------------------\")\n        print(f\"Batch Output file has been written to: {out}\")\n    else:\n        print(\"Output Log:\")\n        for e in batch_errors:\n            print(e)\n        out = \"\"\n    \n    return out","commandVersion":1,"state":"finished","results":{"type":"listResults","data":[],"arguments":{},"addedWidgets":{},"removedWidgets":[],"datasetInfos":[],"metadata":{}},"errorSummary":null,"errorTraceType":null,"error":null,"workflows":[],"startTime":0,"submitTime":0,"finishTime":0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":[],"yColumns":[],"pivotColumns":[],"pivotAggregation":null,"useConsistentColors":false,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","latestUserId":null,"commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"isLockedInExamMode":false,"iPythonMetadata":null,"streamStates":{},"datasetPreviewNameToCmdIdMap":{},"nuid":"8346d01d-2af1-4893-a24d-cfd2aecf2e44"},{"version":"CommandV1","origId":3273046978841380,"guid":"903bddc9-bc08-4973-919d-222a462aa592","subtype":"command","commandType":"auto","position":9.0,"command":"%md\n### Execute Notebook","commandVersion":1,"state":"finished","results":{"type":"raw","data":"","arguments":{},"metadata":{}},"errorSummary":null,"errorTraceType":null,"error":null,"workflows":[],"startTime":0,"submitTime":0,"finishTime":0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":[],"yColumns":[],"pivotColumns":[],"pivotAggregation":null,"useConsistentColors":false,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","latestUserId":null,"commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"isLockedInExamMode":false,"iPythonMetadata":null,"streamStates":{},"datasetPreviewNameToCmdIdMap":{},"nuid":"10672543-da5e-47db-aae1-55a32ca5d79a"},{"version":"CommandV1","origId":3273046978841381,"guid":"91d472d2-8be6-4a6c-9c28-5a59efdd56a2","subtype":"command","commandType":"auto","position":10.0,"command":"if __name__ == '__main__':\n    # Get widget/param value\n    fp = dbutils.widgets.get(\"FileParams\")\n    \n    # Load config from param\n    params_dict = load_batch_config(fp)\n    \n    # If we successfully loaded config, process transformationi\n    if params_dict:\n        output = process_tables()\n    else:\n        raise ValueError(\"Missing or unable to read BATCH configuration/parameters!\") \n        dbutils.notebook.exit(-1)\n","commandVersion":1,"state":"finished","results":{"type":"listResults","data":[],"arguments":{},"addedWidgets":{},"removedWidgets":[],"datasetInfos":[],"metadata":{}},"errorSummary":null,"errorTraceType":null,"error":null,"workflows":[],"startTime":0,"submitTime":0,"finishTime":0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":[],"yColumns":[],"pivotColumns":[],"pivotAggregation":null,"useConsistentColors":false,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","latestUserId":null,"commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"isLockedInExamMode":false,"iPythonMetadata":null,"streamStates":{},"datasetPreviewNameToCmdIdMap":{},"nuid":"8aa980fe-4685-4a32-b962-3016411fe93c"},{"version":"CommandV1","origId":3273046978841382,"guid":"3dbbf6d3-8546-46be-af9f-4c51c932c11c","subtype":"command","commandType":"auto","position":11.0,"command":"%md\n### Display Batch Configuration File Contents","commandVersion":1,"state":"finished","results":{"type":"raw","data":"","arguments":{},"metadata":{}},"errorSummary":null,"errorTraceType":null,"error":null,"workflows":[],"startTime":0,"submitTime":0,"finishTime":0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":[],"yColumns":[],"pivotColumns":[],"pivotAggregation":null,"useConsistentColors":false,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","latestUserId":null,"commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"isLockedInExamMode":false,"iPythonMetadata":null,"streamStates":{},"datasetPreviewNameToCmdIdMap":{},"nuid":"ca5b30cc-9f59-48d4-bd23-83d002b5f52c"},{"version":"CommandV1","origId":3273046978841383,"guid":"6b1c2877-9ea1-4096-92bb-7be146ec104f","subtype":"command","commandType":"auto","position":12.0,"command":"try:\n    pdf = pd.DataFrame(params_dict)\n    display(pdf)\nexcept:\n    print(params_dict)","commandVersion":1,"state":"finished","results":{"type":"listResults","data":[],"arguments":{},"addedWidgets":{},"removedWidgets":[],"datasetInfos":[],"metadata":{}},"errorSummary":null,"errorTraceType":null,"error":null,"workflows":[],"startTime":0,"submitTime":0,"finishTime":0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":[],"yColumns":[],"pivotColumns":[],"pivotAggregation":null,"useConsistentColors":false,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","latestUserId":null,"commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"isLockedInExamMode":false,"iPythonMetadata":null,"streamStates":{},"datasetPreviewNameToCmdIdMap":{},"nuid":"7e2d2513-9a5b-4b93-8376-52d8eb7db472"},{"version":"CommandV1","origId":3273046978841384,"guid":"707c051d-e5da-4e08-8cc8-095f62b380fb","subtype":"command","commandType":"auto","position":13.0,"command":"%md\n### Exit and Return Output","commandVersion":1,"state":"finished","results":{"type":"raw","data":"","arguments":{},"metadata":{}},"errorSummary":null,"errorTraceType":null,"error":null,"workflows":[],"startTime":0,"submitTime":0,"finishTime":0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":[],"yColumns":[],"pivotColumns":[],"pivotAggregation":null,"useConsistentColors":false,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","latestUserId":null,"commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"isLockedInExamMode":false,"iPythonMetadata":null,"streamStates":{},"datasetPreviewNameToCmdIdMap":{},"nuid":"5b3abd76-e7cf-48ed-a4c6-f04d0cb7a29d"},{"version":"CommandV1","origId":3273046978841385,"guid":"1f360012-650f-4eeb-8afb-363a5c9908b7","subtype":"command","commandType":"auto","position":14.0,"command":"# Exit with reference to error file\ndbutils.notebook.exit(output)","commandVersion":1,"state":"finished","results":{"type":"listResults","data":[],"arguments":{},"addedWidgets":{},"removedWidgets":[],"datasetInfos":[],"metadata":{}},"errorSummary":null,"errorTraceType":null,"error":null,"workflows":[],"startTime":0,"submitTime":0,"finishTime":0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":[],"yColumns":[],"pivotColumns":[],"pivotAggregation":null,"useConsistentColors":false,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","latestUserId":null,"commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"isLockedInExamMode":false,"iPythonMetadata":null,"streamStates":{},"datasetPreviewNameToCmdIdMap":{},"nuid":"4452e2e7-d876-4bbd-8321-334cea107e49"}],"dashboards":[],"guid":"f25bcda2-a672-4892-830d-7058e2c10b40","globalVars":{},"iPythonMetadata":{},"inputWidgets":{"Execute Query":{"nuid":"2412808c-eb30-4de2-9bb2-a32f449336e9","currentValue":"TRUE","widgetInfo":{"widgetType":"dropdown","name":"Execute Query","defaultValue":"TRUE","label":null,"options":{"widgetType":"dropdown","choices":["TRUE","FALSE"]}}},"FileParams":{"nuid":"1a48f645-266f-4654-8558-8301ca1b00ec","currentValue":"RAW/CopyToStage.csv","widgetInfo":{"widgetType":"text","name":"FileParams","defaultValue":"","label":null,"options":{"widgetType":"text","validationRegex":null}}}},"notebookMetadata":{"pythonIndentUnit":2}}