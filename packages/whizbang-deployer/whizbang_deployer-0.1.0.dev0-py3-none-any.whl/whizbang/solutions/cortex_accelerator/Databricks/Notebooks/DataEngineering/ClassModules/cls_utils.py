{"version":"NotebookV1","origId":2064366854103208,"name":"cls_utils","language":"python","commands":[{"version":"CommandV1","origId":2064366854103209,"guid":"7a86edc9-435c-4223-8639-cd9251b66ccd","subtype":"command","commandType":"auto","position":2.0,"command":"#\n# Utility class for Batch Ingestion, Batch Transformation, etc.\n# ","commandVersion":1,"state":"finished","results":{"type":"raw","data":"","arguments":{},"metadata":{}},"errorSummary":null,"errorTraceType":null,"error":null,"workflows":[],"startTime":0,"submitTime":0,"finishTime":0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":[],"yColumns":[],"pivotColumns":[],"pivotAggregation":null,"useConsistentColors":false,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","latestUserId":null,"commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"isLockedInExamMode":false,"iPythonMetadata":null,"streamStates":{},"datasetPreviewNameToCmdIdMap":{},"nuid":"bb06a14e-7771-42bf-bd6e-8ed907d79187"},{"version":"CommandV1","origId":2064366854103210,"guid":"4a755053-7e98-4659-8750-11515930ff0d","subtype":"command","commandType":"auto","position":3.0,"command":"from pyspark.sql import DataFrame as SparkDataFrame\nfrom pyspark.sql.types import *\nfrom pyspark.sql.functions import *\nimport re\nimport itertools\nfrom datetime import timedelta, datetime\nimport os\nimport json\nimport pandas as pd\n\nclass Utils:\n    \"\"\"\n    Utility functions for DNA Pipeline processing.\n    \"\"\"\n    # Error logging\n    CRITICAL = 'CRITICAL'\n    ERROR    = 'ERROR'\n    WARNING  = 'WARNING'\n    INFO     = 'INFO'\n    DEBUG    = 'DEBUG'\n    \n    def __init__(self, *args, **kwargs):\n        self.errorLog = []\n        \n        \n    ###########################################################\n    # logger\n    ###########################################################\n    def logger(self, message: str, log_message: bool=False, severity_level: str=INFO) -> None:\n        \"\"\"\n        A simple logger that adds timestamp and severity level to print message.\n        Optionally adds message to instance's \"log\".\n        \"\"\"\n        dtNow = datetime.utcnow()\n        \n        if len(message) > 0:\n            # Simple message to console\n            printMsg = f\"{dtNow:%Y-%m-%d %H:%M:%S}::{message}\"\n\n            # \"Thread-safe\" the print function, or some things print on the same line\n            print(f\"{printMsg}\\n\", end =\"\")\n\n        # Log entry for output\n        if log_message:\n            if severity_level== self.CRITICAL:\n                success = 0\n            else:\n                success = 1\n            \n            # Set end time\n            endTimeUtc = dtNow\n    \n    ###########################################################\n    # create_dataframe_from_file\n    ###########################################################\n    def create_dataframe_from_file (self, filePath: str, fileFormat: str, readOptions: dict=None, fileSchema: str=None, dropDuplicates: bool=True):\n        \"\"\"\n        Loads file.\n         - Reads in file (Supports Parquet, Orc and Csv)\n         - Validates column count in file matches with count in schema parameter\n         - Applies schema parameter to dataframe (overrides loaded file schema)\n         - Removes duplicates.\n        \"\"\"\n        _df = None\n        schema_struct = None\n        errMsg = None\n        fileFormat = fileFormat.lower()\n        fullFilePath = self.get_full_file_path(filePath=filePath)\n        \n        if fileSchema:\n            schema_struct = self.get_file_schema_struct(fileSchema=fileSchema, columnDelimiter=\";\")\n\n        self.logger(f\"Loading source file: {filePath} \") # Materialize the dataframe\n            \n        # Read the file into a dataframe\n        try:\n            if fileFormat == 'csv':\n                # get schema from input file\n                csv_schema = (spark.read\n                        .options(**readOptions)\n                        .csv(fullFilePath)\n                        .schema\n                   )\n                \n                # Read the file\n                _df = (spark.read\n                            .format(fileFormat)\n                            .schema(csv_schema)\n                            .options(**readOptions)\n                            .load(fullFilePath)\n                      )\n            else:\n                _df = (spark.read\n                            .format(fileFormat)\n                            .options(**readOptions)\n                            .load(fullFilePath)\n                      )\n                \n        except Exception as E:\n            errMsg = f\"ERROR! Failed reading file {filePath}  {str(E)[:200].strip()}!\"\n            errMsg = errMsg.replace('\\n','')\n        \n\n        if not errMsg:\n            # Assuming we have read in file:\n            # If schema was passed in, override the dataframe's default schema\n            if schema_struct:\n                # Do a simple comparison of the number of columns, and raise error if they differ\n                if len(schema_struct) != len(_df.schema):\n                    errMsg = f\"Mismatched number of columns between source file ({len(schema_struct)}) and schema passed in ({len(_df.schema)})!\"\n                    _df = None\n                    #return None\n                else:\n                    # Override the dataframe's schema (columns have already been cleaned in get_file_schema())\n                    i = 0\n                    for c in _df.dtypes:\n                        cname_org = c[0]\n                        cname_new = schema_struct[i].name\n                        ctype_org = c[1]\n                        ctype_new = schema_struct[i].dataType\n\n                        _df = ( _df\n                            .withColumnRenamed(cname_org, cname_new)\n                            .withColumn(cname_new, col(cname_new).cast(ctype_new))\n                             )\n                        i += 1\n            # If no schema was passed in, we have to clean up the column names in dataframe\n            else:\n                new_column_name_list = list(map(lambda x: self.clean_string(x, \"\", True), _df.columns))\n\n                # Assign cleaned column names back to existing dataframe\n                _df = _df.toDF(*new_column_name_list)\n\n            if not errMsg:\n                # Drop Duplicates\n                if dropDuplicates:\n                    _df = _df.dropDuplicates()\n\n        #try:\n        #    cnt = _df.count() # Materialize the dataframe\n        #except:\n        if not _df:\n            if not errMsg: #default error if none from above\n                errMsg = f\"ERROR creating dataframe from {filePath}!\"\n            return None, errMsg\n        \n        return _df, None\n    \n    ###########################################################\n    # get_file_schema_struct\n    ###########################################################\n    def get_file_schema_struct(self, fileSchema: str, columnDelimiter: str=\";\") -> StructType:\n        \"\"\"\n        Creates schema StructType from schema parameter (string)\n        Ensures column names are cleaned and converted to lower case.\n        \"\"\"\n        if fileSchema:\n            # Create StructType of file schema for the read\n            schema_list = fileSchema.split(\";\")\n            columns_struct_fields = []\n            \n            # Build field list for StructType\n            for s in schema_list:\n                # Trim leading/trailing spaces\n                s = s.strip()\n                \n                # In case there are spaces in column names, but also a space\n                # between the column and datatype, let's fix that.\n                last_space = s.rfind(\" \")\n                parts = s.split(\" \")\n                if len(parts) > 2:\n                    s = s[:last_space] + \"|\" + s[last_space:]\n                else:\n                    s = s.replace(\" \", \"|\")\n\n                # Get and clean the column name\n                cname = self.clean_string(s.split(\"|\")[0], \"\", True)\n\n                # Get the Spark datatype\n                spark_dtype = self.get_pyspark_datatype_from_sql_type(s.split(\"|\")[1])\n\n                # Add column to list\n                columns_struct_fields.append(StructField(cname, eval(spark_dtype), True))\n\n            # Create the Struct from column list\n            return StructType(columns_struct_fields)\n\n\n    ###########################################################\n    # clean_string\n    ###########################################################\n    def clean_string(self, string_to_clean: str, replace_with: str=\"\", to_lower: bool=False, remove_vowels: bool=False, trim_length: int=0, dedupe: bool=False) -> str:\n        \"\"\"\n        Removes any non-alphanumeric characters from a\n        string, with exception of underscore.\n        Optionally replaces characters with supplied character.\n        Optionally changes string to lower case.\n        Optionally removes vowels from string.\n        Optionally returns only left(n) characters from string.\n        Optionally dedupes the string for consecutively duplicated letters.\n\n        Parameters :\n            string_to_clean : str, mandatory\n            replace_with    : str, optional (default is \"\")\n            to_lower        : boolean, optional (default is False)\n            remove_vowels   : boolean, optional (default is False)\n            trim_length     : int, optional\n            dedupe          : boolean, optional (default is false)\n\n        Output     :\n            Returns cleaned string, optionally with vowels removed\n            and/or trimmed to specified length.\n\n        Examples    :\n            clean_string = strip_string(\"*The!_Field_Name 1\")\n            Outputs:  The_Field_Name1\n        \"\"\"\n        if string_to_clean:\n            regex = re.compile('[^a-zA-Z0-9_]')\n            out_string = regex.sub(replace_with, string_to_clean)\n            if to_lower:\n                out_string = out_string.lower()\n\n            if remove_vowels:\n                out_string = remove_vowels_from_string(out_string)\n\n            if trim_length > 0:\n                out_string = out_string[:trim_length]\n\n            if dedupe:\n                out_string = ''.join(ch for ch, _ in itertools.groupby(out_string))\n        else:\n            out_string = string_to_clean\n\n        return out_string\n\n    ###########################################################\n    # get_full_file_path\n    ###########################################################\n    def get_full_file_path(self, filePath: str) -> str:\n        \"\"\"\n        Appends mount root (mnt) to filepath, ensuring slashes\n        are properly formatted.\n        \"\"\"\n        #Add Databricks/dbfs root mount point to file name\n        args = [\"mnt\", filePath]\n        return Utils.slash_join(args, True)  # handles existing leading slash (or not)\n          \n    ###########################################################\n    # slash_join\n    ###########################################################\n    @staticmethod\n    def slash_join(list_args: list, leading_slash: bool=True) -> str:\n        \"\"\"\n        Joins strings in \"args\" tuple/array together\n        separated with a forward slash.\n        Typically used for ADLS paths.\n        Optionally adds leading forward slash.\n\n        Parameters :\n            args            : str, mandatory\n            leading_slash   : boolean, optional (default is True)\n\n        Output     :\n            Returns string joined by slashes \n\n        Examples    :\n            args = [\"mnt\", \"RAW/file.csv\"]\n            csvfile = slash_join(args, True) \n            Outputs:  /mnt/RAW/file.csv\n        \"\"\"\n        if leading_slash:\n            list_args.insert(0, \"/\")    # Insert a new first element into list\n        \n        all_args = tuple(list_args) # Convert back into Tuple\n        \n        return \"/\".join(arg.strip(\"/\") for arg in all_args)\n\n    ###########################################################\n    # get_pyspark_datatype_from_sql_type\n    ###########################################################\n    def get_pyspark_datatype_from_sql_type(self, sql_data_type:str) -> str:\n        \"\"\"\n        Map SQL to Spark datatype\n\n        Parameters :\n            sql_data_type : str, mandatory\n\n        Output     :\n            Spark data type\n        \"\"\"\n        sql_data_type = sql_data_type.lower()\n        pos = sql_data_type.find(\"(\")\n        if pos > 0:\n            base_col_type = sql_data_type[:sql_data_type.find(\"(\")]\n            sql_size = sql_data_type[sql_data_type.find(\"(\"):].replace(\"(\",\"\").replace(\")\",\"\")\n            # If there aren't any decimals, go ahead and just convert to INTs.\n            if sql_size.find(',0') > 0:\n                base_col_type = \"int\"\n        else:\n            sql_size = \"38,18\" # provide default, just in case\n            base_col_type = sql_data_type\n\n        switcher = {\n            \"varchar\": \"StringType()\",\n            \"char\": \"StringType()\",\n            \"text\": \"StringType()\",\n            \"nvarchar\": \"StringType()\",\n            \"nchar\": \"StringType()\",\n            \"ntext\": \"StringType()\",\n            \"guid\": \"StringType()\",\n            \"binary\": \"StringType()\",    #\"BinaryType()\",\n            \"varbinary\": \"StringType()\", #\"BinaryType()\",\n            \"date\": \"DateType()\",\n            \"time\": \"TimestampType()\",\n            \"datetime\": \"TimestampType()\",\n            \"datetime2\": \"TimestampType()\",\n            \"datetimeoffset\": \"StringType()\", # No Spark equivalent to DTO\n            \"timestamp\": \"BinaryType()\",\n            \"real\": \"FloatType()\",\n            \"double\": f\"DecimalType({sql_size})\",\n            \"decimal\": f\"DecimalType({sql_size})\",\n            \"numeric\": f\"DecimalType({sql_size})\",\n            \"money\": f\"DecimalType({sql_size})\",\n            \"bit\": \"BooleanType()\",\n            \"tinyint\": \"IntegerType()\",\n            \"int\": \"IntegerType()\",\n            \"smallint\": \"IntegerType()\",\n            \"bigint\": \"LongType()\",\n            \"money\": \"DecimalType(19,4)\",\n            \"boolean\": \"BooleanType()\"\n        } \n\n        return(switcher.get(base_col_type, \"StringType()\"))\n\n    ###########################################################\n    # check_if_db_exists\n    ###########################################################\n    def check_if_db_exists(self, db_name, create_if_not_exist=False, delta_location='DELTA'):\n        \"\"\"\n        Checks if database exists\n        Works with Spark 2.x and 3.x (with new column name)\n\n        Parameters :\n            db_name             : str, mandatory\n            create_if_not_exist :  boolean, optional (default=False)\n            delta_location      :  str, optional (default='DELTA')\n\n        Output     :\n            Boolean (existence of table)\n\n        Example    :\n            db_exists = check_if_db_exists(\"myDatabase\", True, 'DELTA')\n        \"\"\"\n\n        # Change to lower case since all tables are stored this way and \n        # for the dataframe query below, it is case sensitive.\n        db_name = db_name.lower()\n\n        if create_if_not_exist:\n            query = f\"CREATE DATABASE IF NOT EXISTS {db_name} LOCATION '/mnt/{delta_location}/{db_name}'\"\n            spark.sql(query)\n\n        # Check if database exists\n        _df = spark.sql(\"show databases\")\n        \n        if _df.select(_df[0]).where(_df[0] == f\"{db_name}\").count():\n            return True\n        else:\n            return False      \n\n    ###########################################################\n    # check_if_table_exists\n    ###########################################################\n    def check_if_table_exists(self, table_name: str, db_name: str=\"default\") -> bool:\n        \"\"\"\n        Checks if table exists in database\n\n        Parameters :\n            table_name : str, mandatory\n            db_name    : str, optional (default is \"default\")\n\n        Output     :\n            Boolean (existence of table)\n\n        Example    :\n            table_exists = check_if_table_exists(\"tableA\", \"myDatabase\")\n        \"\"\"\n\n        # Change to lower case since all tables are stored this way and \n        # for the dataframe query below, it is case sensitive.\n        table_name = table_name.lower()\n\n        try:\n            if spark.sql(f\"show tables in {db_name} like '{table_name}'\").count() > 0:\n                return True\n            else:\n                return False\n        except:\n            pass\n            return False\n          \n    ###########################################################\n    # create_delta_table_from_dataframe\n    ###########################################################\n    def create_delta_table_from_dataframe(self, _df: SparkDataFrame, targetSchema: str, targetTable: str, partitionKey: list=None, writeMode: str=\"overwrite\") -> bool:\n        \"\"\"\n        Creates the initial Delta base table when\n        table does not previously exist.\n        \"\"\"\n        try:\n            if partitionKey:\n                _df.write \\\n                    .format(\"delta\") \\\n                    .mode(writeMode) \\\n                    .option(\"overwriteSchema\", \"true\") \\\n                    .partitionBy(partitionKey) \\\n                    .saveAsTable(f\"{targetSchema}.{targetTable}\")\n            else:\n                _df.write \\\n                    .format(\"delta\") \\\n                    .mode(writeMode) \\\n                    .option(\"overwriteSchema\", \"true\") \\\n                    .saveAsTable(f\"{targetSchema}.{targetTable}\")\n\n            # Set table properties for improved performance in DBR 7.3+\n            # https://docs.microsoft.com/en-us/azure/databricks/delta/optimizations/file-mgmt\n            try:\n                query = f\"\"\"ALTER TABLE {targetSchema}.{targetTable} SET TBLPROPERTIES\n                (\n                 'delta.checkpoint.writeStatsAsStruct' = 'true',\n                 'delta.checkpoint.writeStatsAsJson' = 'false'\n                )\n                \"\"\"\n                spark.sql(query)\n            except:\n                pass\n                \n            return True\n\n        except Exception as E:\n            errMsg = str(E)[:200].replace(\"\\n\",\"\").strip()\n            print(f\"***** ERROR creating table {targetSchema.upper()}.{targetTable.upper()}: {errMsg}\")\n            pass\n            return False\n\n    ###########################################################\n    # trim_string_columns\n    ###########################################################\n    def trim_string_columns(self, _df: SparkDataFrame) -> SparkDataFrame:\n        \"\"\"\n        Trims string columns since some data sources send CHAR datatypes.\n        \"\"\"\n        # Make sure string column data is trimmed (some datasets use \"char\" datatypes)\n        for c, t in _df.dtypes:\n            if t == \"string\":\n                _df = _df.withColumn(c, ltrim(rtrim(_df[c])))\n\n        return _df\n    \n    ###########################################################\n    # file_exists\n    ###########################################################\n    def file_exists(self, file_name: str) -> bool:\n        \"\"\"\n        Performs simple filesystem utility \"ls\" command\n        and returns whether it exists or not.\n        Leave to code whether an exception is actually raised or not.\n\n        Parameters  :\n            file_name    :  str, mandatory\n\n        Output      :\n            Boolean indicator if file is found.\n        \"\"\"\n        found = False\n        try:\n            dbutils.fs.ls(file_name)\n            found = True\n        except:  # Didn't find the file\n            pass\n\n        return(found)  \n\n    ###########################################################\n    # dict_from_dataframe\n    ###########################################################\n    def dict_from_dataframe(self, _df: SparkDataFrame) -> dict:\n        \"\"\"\n        Creates a dictionary from a dataframe, whereby\n        allowing key name lookups, as well as\n        iterating over values.\n\n        Parameters  :\n            _df  :  str, mandatory\n\n        Output      :\n            Dictionary with \"'ColumnName': ([list of values])\".\n\n        Example     :\n            dfa_dict = dict_from_dataframe(dfa)\n\n        \"\"\"\n        colDict = dict(zip(_df.schema.names, zip(*_df.collect())))\n        return colDict if colDict else dict.fromkeys(_df.schema.names, ())\n\n    ###########################################################\n    # spark_to_sql_datatype\n    ###########################################################\n    def spark_to_sql_datatype(self, sql_data_type):\n        \"\"\"\n        Map Spark to SQL datatype\n\n        Parameters :\n            sql_data_type : str, mandatory\n\n        Output     :\n            SQL data type\n        \"\"\"\n        switcher = {\n            \"tinyint\": \"tinyint\",\n            \"smallint\": \"smallint\",\n            \"int\": \"int\",\n            \"bigint\": \"bigint\",\n            \"boolean\": \"bit\",\n            \"double\": \"float\",\n            \"float\": \"real\",\n            \"string\": \"nvarchar\",\n            \"nvarchar\": \"nvarchar\",\n            \"binary\": \"varbinary(max)\",\n            \"timestamp\": \"datetime2\",\n            \"date\": \"datetime2\",\n            \"decimal\": \"decimal\"\n        }\n        return(switcher.get(sql_data_type, \"nvarchar(255)\"))\n\n    ###########################################################\n    # spark_to_snowflake_datatype\n    ###########################################################\n    def spark_to_snowflake_datatype(self, sql_data_type):\n        \"\"\"\n        Map Spark to Snowflake datatypes\n        Refs:\n            https://docs.snowflake.com/en/user-guide/spark-connector-use.html#moving-data-from-spark-to-snowflake\n            https://docs.snowflake.com/en/sql-reference/data-types.html\n\n        Parameters :\n            sql_data_type : str, mandatory\n\n        Output     :\n            Snowflake data type\n        \"\"\"\n        sql_data_type = sql_data_type.lower()\n        pos = sql_data_type.find(\"(\")\n        if pos > 0:\n            base_col_type = sql_data_type[:sql_data_type.find(\"(\")]\n            sql_size = sql_data_type[sql_data_type.find(\"(\"):].replace(\"(\",\"\").replace(\")\",\"\")\n            # If there aren't any decimals, go ahead and just convert to INTs.\n            if sql_size.find(',0') > 0:\n                base_col_type = \"int\"\n        else:\n            sql_size = \"38,18\" # provide default, just in case\n            base_col_type = sql_data_type\n\n        switcher = {\n            #Pyspark types\n            \"arraytype\": \"VARIANT\",\n            \"binarytype\": \"BINARY\",  #Snowflake documentation says BinaryType not supported\n            \"booleantype\": \"BOOLEAN\",\n            \"bytetype\": \"INTEGER\",\n            \"datetype\": \"DATE\",\n            \"decimaltype\": f\"DECIMAL({sql_size})\",\n            \"doubletype\": \"DOUBLE\",\n            \"floattype\": \"FLOAT\",\n            \"integertype\": \"INTEGER\",\n            \"longtype\": \"INTEGER\",\n            \"maptype\": \"VARIANT\",\n            \"shorttype\": \"INTEGER\",\n            \"stringtype\": \"VARCHAR\", #Length can be specified, or defaults to max length (16MB - 8MB unicode)\n            \"structtype\": \"VARIANT\",\n            \"timestamptype\": \"TIMESTAMP\",\n            # Hive/SQL data types\n            \"tinyint\": \"INTEGER\",\n            \"smallint\": \"INTEGER\",\n            \"int\": \"INTEGER\",\n            \"bigint\": \"INTEGER\",\n            \"boolean\": \"BOOLEAN\",\n            \"double\": \"DOUBLE\",\n            \"float\": \"FLOAT\",\n            \"string\": \"VARCHAR\",\n            \"nvarchar\": \"VARCHAR\",\n            \"binary\": \"BINARY\",\n            \"timestamp\": \"TIMESTAMP\",\n            \"date\": \"DATE\",\n            \"decimal\": f\"DECIMAL({sql_size})\"\n        }\n        return(switcher.get(base_col_type, \"VARCHAR(4000)\"))\n\n    ###########################################################\n    # snowflake_to_spark_datatype\n    ###########################################################\n    def snowflake_to_spark_datatype(self, sf_data_type):\n        \"\"\"\n        Map Snowflake to Spark datatypes\n        Refs:\n            https://docs.snowflake.com/en/user-guide/spark-connector-use.html#moving-data-from-spark-to-snowflake\n            https://docs.snowflake.com/en/sql-reference/data-types.html\n\n        Parameters :\n            sf_data_type : str, mandatory\n\n        Output     :\n            Spark data type\n        \"\"\"\n        sf_data_type = sf_data_type.upper()\n        if sf_data_type.find(',0') > 0:\n            sf_data_type = \"INTEGER\"\n        elif sf_data_type.find(\"(\") > 0:\n            sf_data_type = sf_data_type[:sf_data_type.find(\"(\")]\n\n        switcher = {\n            #Pyspark types\n            \"VARIANT\":  \"ArrayType()\",\n            \"BOOLEAN\":  \"BooleanType()\",\n            \"DATE\":     \"DateType()\",\n            \"DECIMAL\":  \"DecimalType()\",\n            \"DOUBLE\":   \"DoubleType()\",\n            \"FLOAT\":    \"FloatType()\",\n            \"INTEGER\":  \"IntegerType()\",\n            \"NUMBER\":   \"DecimalType()\",\n            \"VARCHAR\":  \"StringType()\",\n            \"TIMESTAMP\":\"TimestampType()\",\n            \"TIMESTAMP_NTZ\":\"TimestampType()\"\n        }\n        return(switcher.get(sf_data_type, \"StringType()\"))\n\n    ###########################################################\n    # get_datatype_precision\n    ###########################################################\n    \"\"\"\n    Simple function to return length/precision/scale of a datatype\n    \"\"\"\n    def get_datatype_precision(self, data_type: str):\n        ret_val = \"\"\n        try:\n            pos = data_type.find(\"(\")\n            if pos > 0:\n                ret_val = data_type[pos:]\n        except:\n            pass\n\n        return ret_val\n\n    ###########################################################\n    # data_validation_check\n    ###########################################################\n    def data_validation_check(self, _df: SparkDataFrame, candidate_key_list: list, null_check: bool, duplicate_check: bool, remove_duplicates: bool=False, remove_nulls: bool=False):\n        \"\"\"\n        Performs basic validation tests on a Spark dataframe\n        \n        Parameters :\n            candidate_key_list : list, mandatory\n            null_check         : boolean, mandatory\n            duplicate_check    : boolean, mandatory\n            remove_duplicates  : boolean, optional\n            remove_nulls       : boolean, optional\n\n        Output     :\n            validation_passed  : boolean, Validation passed/failed test(s)\n            _df                : dataframe, either as passed in, or optionally with duplicates and null candidate keys removed\n            error_message      : string, One or more error messages indicating validations that failed, passed back to calling code\n        \"\"\"\n        \n        validation_passed = True\n        dupes_passed = True\n        nulls_passed = True\n        error_message = None\n        \n        if not _df.count() > 0:\n            validation_passed = False\n            error_message = \"Source file contains no data!\"\n        else:\n            #First let's make sure columns in candidate key exist in source dataframe\n            df_cols = _df.columns\n\n            if not all(item in df_cols for item in candidate_key_list):\n                validation_passed = False\n                error_message = \"One or more columns in candidate key are missing from source data!\"\n    \n            if not error_message:\n                if duplicate_check:\n                    if _df.groupBy(candidate_key_list).count().filter(\"count > 1\"):\n                        if remove_duplicates:\n                            _df = _df.dropDuplicates(candidate_key_list)\n                        else:\n                            dupes_passed = False\n                            error_message = \"Duplicate Candidate Keys were found in source data!\"\n\n                if null_check:\n                    if _df.select(concat(*candidate_key_list)):\n                        if remove_nulls:\n                             _df = _df.filter(concat(*candidate_key_list).isNotNull())\n                        else:\n                            if _df.filter(concat(*candidate_key_list).isNull()).count() > 0:\n                                nulls_passed = False\n                                err = \"Nulls were found in Candidate Key!\"\n                                if error_message:\n                                    error_message = error_message + \"; \" + err\n                                else:\n                                    error_message = err\n        \n            validation_passed = dupes_passed and nulls_passed\n            \n        return(validation_passed, _df, error_message)\n      \n    @staticmethod\n    def check_alive(spark_conn) -> bool:\n        \"\"\"Check if connection is alive. ``True`` if alive, ``False`` if not\"\"\"\n        try:\n            get_java_obj = spark_conn._jsc.sc().getExecutorMemoryStatus()\n            return True\n        except Exception:\n            return False\n\n    @staticmethod\n    def get_number_of_executors(spark_conn) -> int:\n        if not Utils.check_alive(spark_conn):\n            raise Exception('Unexpected Error: Spark Session has been killed')\n        try:\n            return spark_conn._jsc.sc().getExecutorMemoryStatus().size()\n        except:\n            raise Exception('Unknown error')\n\n    @staticmethod\n    def get_number_of_workers() -> int:\n        cluster_executors = utilsLib.get_number_of_executors(sc)\n        cluster_workers = cluster_executors - 1\n        return cluster_workers\n\n    @staticmethod\n    def get_cores_per_node() -> int:\n        return os.cpu_count()\n\n    @staticmethod\n    def get_cluster_cores() -> int:\n        execs = Utils.get_number_of_executors(sc)\n        cores = Utils.get_cores_per_node()\n        return execs * cores\n\n    @staticmethod\n    def get_cluster_memory() -> int:\n        context = json.loads(dbutils.notebook.entry_point.getDbutils().notebook().getContext().toJson())\n        try:\n            tot_mem = context.get(\"tags\").get(\"clusterMemory\")\n        except:\n            tot_mem = 0\n        return tot_mem\n\n\n    @staticmethod\n    def get_executor_memory() -> int:\n        conf = sc.getConf().getAll()\n        exec_mem = 0\n        try:\n            exec_mem = int([x[1] for x in conf if x[0] == \"spark.executor.memory\"][0].replace(\"m\",\"\"))\n        except:\n            pass\n        return exec_mem\n      \n    @staticmethod\n    def get_file_size(file_name: str) -> int:\n        \"\"\"\n        Input:   Full path of file name, e.g. /RAW/LINK/YARDI/Person/Person.csv\n                 Do not include mount point reference.\n                 \n        Output:  Size of file\n        \"\"\"\n        file_size = 0\n        args = [\"mnt\", file_name]\n        the_file = Utils.slash_join(list_args=args, leading_slash=True)\n#        try:\n        file_properties = dbutils.fs.ls(the_file)\n        file_size = int(file_properties[0][2])\n#         except:\n#             file_size = 0\n#             pass\n        return file_size\n \n    @staticmethod\n    def get_table_metrics(schemaName: str, tableName: str):\n        out_dict = {}\n        _tbl = f\"{schemaName}.{tableName}\"\n\n        try:\n            tbl_info = spark.sql(f\"describe detail {_tbl}\").collect()\n            if tbl_info:\n                table_file_count = tbl_info[0].numFiles\n                table_size_bytes = tbl_info[0].sizeInBytes\n            else:\n                table_file_count = 0\n                table_size_bytes = 0\n        except:\n            table_file_count = 0\n            table_size_bytes = 0\n            pass\n        \n        try:\n            part_info = spark.sql(f\"show partitions {_tbl}\").collect()\n            if part_info:\n                table_partitions = len(part_info)\n            else:\n                table_partitions = 0\n        except:\n            table_partitions = 0\n            pass\n\n        out_dict[\"tablefilecount\"] = table_file_count\n        out_dict[\"tablesizebytes\"] = table_size_bytes\n        out_dict[\"tablepartitions\"] = table_partitions\n        return out_dict  \n\n    @staticmethod\n    def get_table_metrics2(rowNum: int, schemaName: str, tableName: str, fileName: str):\n        out_dict = {}\n        _tbl = f\"{schemaName}.{tableName}\"\n\n        #fsize = ws_api_client.get_file_size(file)\n        try:\n            tbl_info = spark.sql(f\"describe detail {_tbl}\").collect()\n            if tbl_info:\n                table_file_count = tbl_info[0].numFiles\n                table_size_bytes = tbl_info[0].sizeInBytes\n            else:\n                table_file_count = 0\n                table_size_bytes = 0\n        except:\n            table_file_count = 0\n            table_size_bytes = 0\n            pass\n        \n        try:\n            part_info = spark.sql(f\"show partitions {_tbl}\").collect()\n            if part_info:\n                table_partitions = len(part_info)\n            else:\n                table_partitions = 0\n        except:\n            table_partitions = 0\n            pass\n\n        out_dict[\"rownum\"] = rowNum\n        out_dict[\"metrics\"] = [table_file_count, table_size_bytes, table_partitions, fileName]\n        return out_dict  \n      \n    @staticmethod\n    def log_file_writer(file_path, object_to_write, line_separator=None) -> bool:\n        \"\"\"\n        Log File Writer\n        Writes string or list to file\n        Optionally separates items in list with provided separator\n        \"\"\"\n        success = False\n        \n        args = []\n        \n        if \"dbfs\" not in file_path.lower():\n            args.append(\"dbfs\")\n        if \"mnt\" not in file_path.lower():\n            args.append(\"mnt\")\n        \n        args.append(file_path)\n        \n        dbfs_path = utilsLib.slash_join(args, True)\n        \n        try:\n            file_handle = open(dbfs_path, \"w\")\n            \n            # Write list or string, as appropriate\n            if isinstance(object_to_write, list):\n                for item in object_to_write:\n                    file_handle.write(f\"{item}\\n\")\n                    if line_separator:\n                        file_handle.write(line_separator)\n            else:\n                file_handle.write(f\"{object_to_write}\\n\")\n                \n            print(f\"Log file written to: {file_path}\")\n            success = True\n        except:\n            print(f\"Error writing to file {file_path}\")\n            success = False\n        finally:\n            if file_handle:\n                file_handle.close()\n        return success\n      \n    @staticmethod\n    def json_to_dataframe(json_str: str) -> SparkDataFrame:\n        # Use Pandas for easy conversion of JSON to dataframe\n        # Then convert to Spark Dataframe\n        _df = None\n        try:\n            # Use Pandas for easy conversion of JSON to dataframe\n            _pdf = pd.DataFrame(json_str)\n            # Then convert to Spark Dataframe\n            _df = spark.createDataFrame(_pdf)\n        except Exception as E:\n            print(f\"ERROR creating dataframe from JSON string: {json_str} :: {str(E)[:200].strip()}\")\n            pass\n        return _df\n      \n    @staticmethod\n    def run_sql_query (query, execute_query=True, print_output=True):\n        \"\"\"\n        Generic function to run a Spark SQL query\n        and handle the error.\n\n        Parameters:\n            query :  str, mandatory\n            execute_query :  boolean, optional\n\n        Output:\n            boolean\n        \"\"\"\n        if execute_query:\n            try:\n                spark.sql(query)\n            except Exception as E:\n                if print_output:\n                    print(f\"WARNING: Error executing query: {query}\")\n                    print(f\"\\t{E}\")\n                pass\n                return False\n\n        return True\n\n    ###########################################################\n    # write_output_file\n    ###########################################################\n    @staticmethod\n    def write_output_file(source_file: str, batch_errors: list, column_list: list):\n        \"\"\"\n        Generate the output file with an entry for each file processed\n        \"\"\"\n        # Quoting constant for writing CSV with Pandas \n        QUOTE_NONNUMERIC = 2\n\n        #col_list = ['DeltaSchema','DeltaTable','DeltaStartTimeUtc','DeltaEndTimeUtc','DeltaAdditionalOutput','DeltaSuccess','DeltaError']\n        # Return batch errors as JSON \n        if batch_errors:\n            pdf = pd.DataFrame(batch_errors, columns = column_list).reindex()\n\n            # Convert column to Int\n            pdf['Success'] = pdf['Success'].astype(int)\n\n            # Generate error log file name, based on input file\n            output_file = source_file.replace(\".csv\", \"_OUTPUTLOG.csv\")\n\n            # Add dbfs/mnt/ to file path\n            saveLocationParts = [\"dbfs\", \"mnt\", output_file]\n            saveLocation = utilsLib.slash_join(saveLocationParts, True) \n\n            # Write out file using Pandas for a single CSV file (without extra Spark metadata folder/files)\n            if not pdf.empty:\n                pdf[column_list].to_csv(saveLocation, sep=\"|\", index=False, quotechar='\"', quoting=QUOTE_NONNUMERIC)\n                output_file = output_file.replace(\"/dbfs/mnt/\", \"\")\n        else:\n            output_file = \"\"\n\n        return output_file\n\n    ###########################################################\n    # get_value_from_connection_string\n    ###########################################################\n    @staticmethod\n    def get_value_from_connection_string(conn_str, cs_key, delimiter=\";\"):\n        \"\"\"\n        Description:\n          Converts a connection string to dictionary,\n            then searches for a particular key\n          This is a case-insensitive search.\n\n        Params:\n          conn_str  : string (Mandatory) The connection string\n          key       : string (Mandatory) The key value you're searching for\n          delimiter : string (Optional)  Delimiter used in connection string\n\n        Example:\n          db_name = get_value_from_connection_string(\"Database=db1, Schema=sch1, User=usr1, Pw=pw1\", \"Database\", \",\")\n          print(db_name)\n        \"\"\"\n        cs_dict = {}\n        cs_key = cs_key.lower()\n\n        # Prepare string to convert into dictionary\n        conn_str = conn_str.replace(\"=\", '\":\"').replace(delimiter, '\",\"').replace(',\"\"', \"\")\n        conn_str = '{\"' + conn_str + '\"}'\n\n        # Create dictionary from string\n        cs_dict = ast.literal_eval(conn_str)\n\n        # Convert keys to lower case for case-insensitve search\n        cs_dict = {k.lower(): v for k, v in cs_dict.items()}\n\n        # Get the value for the desired key\n        ret_val = cs_dict.get(cs_key)\n        return ret_val\n","commandVersion":1,"state":"finished","results":{"type":"raw","data":"","arguments":{},"metadata":{}},"errorSummary":null,"errorTraceType":null,"error":null,"workflows":[],"startTime":0,"submitTime":0,"finishTime":0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":[],"yColumns":[],"pivotColumns":[],"pivotAggregation":null,"useConsistentColors":false,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","latestUserId":null,"commandTitle":"Utils Class","showCommandTitle":true,"hideCommandCode":false,"hideCommandResult":false,"isLockedInExamMode":false,"iPythonMetadata":null,"streamStates":{},"datasetPreviewNameToCmdIdMap":{},"nuid":"a6e9e02a-80b4-45e7-a73b-27e587ca0621"}],"dashboards":[],"guid":"b206a99d-5c48-48a4-8e66-c953396be739","globalVars":{},"iPythonMetadata":null,"inputWidgets":{},"notebookMetadata":{"pythonIndentUnit":2}}